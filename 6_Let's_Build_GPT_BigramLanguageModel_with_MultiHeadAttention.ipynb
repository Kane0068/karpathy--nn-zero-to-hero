{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Hikayemiz: Bir Sınıftaki Öğrencilerin Fısıltı Oyunu 🤫\n",
        "Haydi, bir sınıf hayal edelim. Bu sınıftaki her bir öğrenci, bir cümlenin içindeki bir kelimeyi temsil ediyor.\n",
        "\n",
        "Örneğin batch_size = 4: Sınıfta 4 tane birbirinden bağımsız \"oyun grubu\" var (batch).block_size = 8: Her grupta 8 öğrenci (kelime) var. Bu öğrenciler sırayla oturuyorlar. Cümlenin sırası gibi.\n",
        "n_embd = 64: Her öğrencinin kendine ait 64 farklı \"özelliği\" var. Mesela bir öğrencinin \"neşeli\", \"çalışkan\", \"yorgun\" gibi 64 farklı özelliği olabilir. Bu, kelimenin anlamını taşıyan bir vektör.\n",
        "Kodumuzdaki x değişkeni, işte bu 4 grubun, 8'er öğrenciden oluşan ve her öğrencinin 64 özelliği olan sınıf listesidir.\n",
        "\n",
        "--------------\n",
        "\n",
        "Adım 1: Herkesin Üç Farklı Şapka Takması (Key, Query, Value)\n",
        "Sınıfta bir oyun oynayacağız. Oyunun adı \"Anlamı Zenginleştirme\". Her öğrencinin (kelimenin) diğer öğrencilere bakarak kendi anlamını daha iyi kavraması gerekiyor.\n",
        "\n",
        "Bunun için her öğrenciye üç farklı \"rol\" veya \"şapka\" veriyoruz. Bu şapkaları takınca, sahip oldukları 64 özellik, daha küçük ve amaca yönelik 4 özelliğe dönüşüyor (head_size = 4).\n",
        "\n",
        "-----------------\n",
        "Sorgu Şapkası (Query ❓): Bir öğrenci bu şapkayı taktığında, diğerlerine sormak istediği soruyu hazırlar. Bu, \"Ben kiminle ilgiliyim? Bana kimin anlamı lazım?\" diye bağırmak gibidir.\n",
        "q = query(x) -> Her öğrenci sorgu şapkasını taktı ve elinde bir \"soru kartı\" (q) oluştu.\n",
        "\n",
        "Anahtar Şapkası (Key 🔑): Bir öğrenci bu şapkayı taktığında, kendisinin ne hakkında olduğunu özetleyen bir \"anahtar kelime\" veya \"etiket\" oluşturur. Bu, \"Benim konum bu! Eğer benimle ilgili bir şey arıyorsan, bu etikete bak!\" demek gibidir.\n",
        "k = key(x) -> Her öğrenci anahtar şapkasını taktı ve boynuna bir \"etiket\" (k) astı.\n",
        "\n",
        "Değer Şapkası (Value 🎁): Bu şapkayı takan öğrenci ise, eğer birisi ona ilgi gösterirse sunacağı \"bilgi paketini\" hazırlar. Bu, \"Eğer bana dikkat etmeye karar verirsen, sana vereceğim değerli bilgi bu.\" demek gibidir.\n",
        "v = value(x) -> Her öğrenci değer şapkasını taktı ve elinde bir \"hediye paketi\" (v) tutuyor.\n",
        "\n",
        "Her öğrenci, elindeki \"soru kartını\" (q) kaldırır ve sınıftaki diğer tüm öğrencilerin boynundaki \"etiketlere\" (k) bakar.\n",
        "\n",
        "q @ k.transpose(...): Bu matematiksel işlem (@), bir matris çarpımıdır ama biz buna \"ilgi ölçer\" diyelim. Her öğrencinin sorusu, diğer tüm öğrencilerin etiketleriyle \"çarpılır\" ve bir uyumluluk puanı ortaya çıkar.\n",
        "Eğer bir öğrencinin sorusu, başka bir öğrencinin etiketiyle çok uyumluysa, aralarındaki puan (wei, yani weight/ağırlık) yüksek olur. Uyumsuzsa, puan düşük olur.\n",
        "Sonuçta elimizde ne var? 8x8'lik bir tablo! Bu tablo, her öğrencinin diğer her bir öğrenciye (kendisi de dahil) ne kadar ilgi duyduğunu gösteren bir \"ilgi haritası\"dır.\n",
        "Adım 3: Geleceği Görmek Yasak! (Maskeleme)\n",
        "Bu oyunda önemli bir kural var: Öğrenciler kendilerinden sonra gelen öğrencilerden kopya çekemez! Yani bir kelime, cümlenin ilerisindeki bir kelimenin anlamından etkilenemez. Sadece kendinden öncekilere bakabilir.\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "torch.tril(...): Bu kod, sihirli bir \"kural maskesi\" (tril) yaratır. Bu maske, ilgi haritamızla aynı boyutta (8x8) ve sadece sol alt üçgeni 1'lerle dolu. Geri kalanı 0. (1 = bakabilirsin, 0 = bakamazsın).\n",
        "masked_fill(...): Bu komutla kuralı uyguluyoruz. İlgi haritamızdaki (wei) bakılması yasak olan tüm yerleri (maskenin 0 olduğu yerleri) negatif sonsuz (-inf) puanı ile doldururuz. Bu, \"Oraya bakman o kadar yasak ki, ihtimali bile yok!\" demektir.\n",
        "Artık ilgi haritamız, her öğrencinin sadece kendisi ve kendinden öncekilere olan ilgi puanlarını içeriyor.\n",
        "\n",
        "Adım 4: İlgi Dağılımı (Softmax)\n",
        "Elimizde ham ilgi puanları var. Ama bir öğrencinin toplamda %100'lük bir \"dikkat\" kaynağı var. Bu kaynağı, ilgi duyduğu kişilere nasıl dağıtacak? İşte burada Softmax devreye giriyor.\n",
        "\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "Softmax, ham ve anlamsız puanları (örneğin: 2.3, -1.5, 10.8) alır ve onları güzel bir olasılık dağılımına dönüştürür. Yani, her öğrenci için ilgi duyduğu diğer öğrencilere verdiği dikkat yüzdelerini hesaplar.\n",
        "\n",
        "Puanı yüksek olan öğrenci, dikkatin büyük bir kısmını alır (%70 gibi).\n",
        "Puanı düşük olan öğrenci, az bir dikkat alır (%5 gibi).\n",
        "Puanı negatif sonsuz olan (gelecekteki öğrenciler) ise %0 dikkat alır.\n",
        "Sonuçta wei tablomuz, artık her öğrencinin dikkatini geçmişteki hangi öğrencilere yüzde kaç oranında dağıttığını gösteren net bir \"dikkat haritası\" haline geldi.\n",
        "\n",
        "Adım 5: Bilgileri Toplama ve Anlamı Zenginleştirme\n",
        "Geldik son adıma! Artık her öğrenci kimlere ne kadar dikkat edeceğini biliyor. Şimdi, o dikkat oranında onlardan bilgi toplama zamanı.\n",
        "\n",
        "out = wei @ v\n",
        "Her öğrenci, elindeki \"dikkat haritasına\" (wei) bakar.\n",
        "Diyelim ki 3. öğrenci, 1. öğrenciye %20, 2. öğrenciye ise %80 dikkat etmeye karar verdi.\n",
        "O zaman 1. öğrencinin elindeki \"hediye paketinden\" (v) %20 alır, 2. öğrencinin hediye paketinden ise %80 alır.\n",
        "Bu topladığı bilgileri birleştirerek kendi yeni zenginleştirilmiş bilgisini (out) oluşturur.\n",
        "İşte bu out değişkeni, her öğrencinin (kelimenin) sadece kendi başına değil, cümlenin bağlamına (kendinden önceki kelimelere) dikkat ederek oluşturduğu yeni ve daha derin anlamıdır!"
      ],
      "metadata": {
        "id": "QsdMdOt46h_R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.Bölüm:Projenin Planı ve Malzemeler(Hyperparameters & Data)"
      ],
      "metadata": {
        "id": "s0pAA3Y_2pKZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##A.Kütüphaneleri dahil etme"
      ],
      "metadata": {
        "id": "AoSzQyGRx8MO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "qBOBLxOu7EMv"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##B.Hiperparametreler"
      ],
      "metadata": {
        "id": "BM3v-S8sx4XO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 16 # Paralel olarak kaç tane bağımsız metin dizisini işleyeceğiz?\n",
        "block_size = 32 # Tahminler için maximum bağlam uzunluğu (Ne kadar geçmişe bakabilir?)\n",
        "max_iters = 5000 # Toplamda kaç eğitim adımı(ders) yapılacak?\n",
        "eval_interval = 100 # Her 100 derste bir öğrenme durumunu kontrol et(sınav yap)\n",
        "learning_rate = 1e-3 # Öğrenme hızı(Her hatadan sonra ne kadar büyük bir düzeltme yapacak?)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' # Dersleri GPU'da mı(hızlı) yoksa CPU'da mı(yavaş) işleyeceğiz?\n",
        "eval_iters = 200 # Sınavda kaç soru sorulacak?\n",
        "n_embd = 64 # Her bir karakterin/kelimenin anlamını kaç sayıyla temsil edeceğiz?(Anlam Zenginliği)\n",
        "n_head = 4 # Kaç tane 'dikkat başlığı' (farklı bakış açısı) olacak\n",
        "n_layer = 4 # Kaç tane 'transformatör bloğu' (düşünme katmanı) üst üste konulacak?\n",
        "dropout = 0.0 # Ezberlemeyi önlemek için ne kadar 'unutkanlık' ekleyeceğiz ?(% 0, yani şimdilik hiç)"
      ],
      "metadata": {
        "id": "Edoy91D78X9a",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Be0dIzI5HaR",
        "outputId": "192c0ed1-0dde-479e-e232-bb128683a7de"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7e277cfd12b0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##C.Veri Setini Yükleme"
      ],
      "metadata": {
        "id": "dpDyfSW_xy0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAqU5Uz-xi9v",
        "outputId": "6ae06f81-3341-46e2-83a3-a5648bde62eb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-17 19:16:51--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2025-06-17 19:16:51 (83.7 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##D.Karakter Seti"
      ],
      "metadata": {
        "id": "CTgih-GzyB-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Metindeki tüm benzersiz karakterleri buluyoruz.Bu bizim 'alfabemiz'\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars) # Alfabemizde kaç karakter var ?\n",
        "\n",
        "# Karakterileri sayılara (stoi) ve sayıları karakterlere çeviren (itos) sözlükler oluşturuyoruz\n",
        "# Çünkü bilgisayar harflerle değil sayılarla çalışır\n",
        "stoi = {ch : i for i , ch in enumerate(chars)}\n",
        "itos = {i : ch for i , ch in enumerate(chars)}\n",
        "encode = lambda s : [stoi[c] for c in s]        # Bir metni sayı dizisine çevirir\n",
        "decode = lambda l : ''.join([itos[i] for i in l])     # Bir sayı dizisini metne çevirir\n",
        "\n",
        "\n",
        "# Veriyi Eğitim ve Sınav(Validasyon) olarak ikiye ayırıyoruz\n",
        "\n",
        "data = torch.tensor(encode(text) , dtype = torch.long)\n",
        "n = int(0.9*len(data))   # Verinin %90 'ı eğitim , % 10 sınav için\n",
        "train_data = data[:n]\n",
        "val_data= data[n:]\n",
        "\n"
      ],
      "metadata": {
        "id": "u_6D_yrg7ggM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.Bölüm : Ders ve Sınav Hazırlığı"
      ],
      "metadata": {
        "id": "KcMnk_wE21h_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##A.Veri Yükleme / Ders Hazırlama Fonksiyonu"
      ],
      "metadata": {
        "id": "DwF9dd4CzqRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# bu fonksiyon her ders için metinden rastgele parça seçer\n",
        "\n",
        "def get_batch(split):\n",
        "  # Eğitim veya sınav setinden veri seçilir\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  # Metin içinden rastgele başlangıç noktaları seçilir (batch_size kadar)\n",
        "  ix = torch.randint(len(data) - block_size,(batch_size,))\n",
        "  # Seçilen noktalardan block_size uzunluğunda metin parçaları(x) alınır\n",
        "  x = torch.stack([data[i : i + block_size] for i in ix])\n",
        "  # Hedefler (y) , x 'in bir sonraki karakteridir.Modelden bunu tahmin etmesini isteyeceğiz.\n",
        "  y = torch.stack([data[i+1: i + block_size+1] for i in ix])\n",
        "  return x , y\n",
        "\n",
        "  #Hikayesi: Öğrenciye bir metin parçası (x) veriyoruz ve \"Bu metindeki her bir karakterden sonra hangi karakter gelir?\" diye soruyoruz.\n",
        "  # Doğru cevaplar da y oluyor."
      ],
      "metadata": {
        "id": "-Vtg-eVJzwep"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##B.Sınav Yapma Fonksiyonu - Kayıp Tahmin\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zgezjbD_1INn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bu fonksiyon , modelin ne kadar iyi öğrendiğini ölçer.\n",
        "\n",
        "@torch.no_grad()  # Bu blokta öğrenme(gradyan hesaplama) yapma , sadece değerlendir demek için dekoratörü kullandık.\n",
        "\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval() # modeli sınav moduna al\n",
        "\n",
        "  for split in ['train','val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X , Y = get_batch(split) # Bir derslik veri al\n",
        "      logits , loss = model(X ,Y) # Modelin tahminini ve hatasını al\n",
        "      losses[k] = loss.item()     # Hatam miktarını kaydet\n",
        "    out[split] = losses.mean()    # Ortalama hatayı hesapla\n",
        "  model.train()   # Modeli tekrar \"Öğrenme Moduna\" al\n",
        "  return out\n",
        "\n",
        "#Hikayesi: Modelin öğrenmesini geçici olarak durdururuz.\n",
        "#Ona bir sürü soru (eval_iters kadar) sorarız ve ortalama ne kadar hata yaptığına bakarız.\n",
        "#Bu, ezber mi yapıyor yoksa gerçekten öğreniyor mu anlamamızı sağlar."
      ],
      "metadata": {
        "id": "k8nGhj041LqN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.Bölüm : Modelin İnşaası (Beynin Parçaları)\n",
        "- Şimdi modelin kendisini, yani \"dijital beyni\" inşa ediyoruz. Bu beyin, katman katman çalışan parçalardan oluşur.\n",
        "\n"
      ],
      "metadata": {
        "id": "eu3qPS8x2lRS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##A.Head Sınıfı : Tek bir Dikkat Kafası\n",
        "- Bir kelimenin/karakterin, kendinden öncekilere nasıl dikkat edeceğini belirler."
      ],
      "metadata": {
        "id": "VR_07SLF3a3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "  def __init__(self,head_size):\n",
        "    super().__init__()\n",
        "\n",
        "    # Query , Key , Value katmanları : Her kelimeye/karaktere üç farklı şapka takar\n",
        "\n",
        "    self.key = nn.Linear(n_embd , head_size , bias = False)\n",
        "    self.query = nn.Linear(n_embd , head_size , bias = False)\n",
        "    self.value = nn.Linear(n_embd , head_size , bias = False)\n",
        "\n",
        "    # Geleceğe bakmayı engelleyen maske --- Not : İlişkileri sadece geçmişe bakarak öğrenecek.Geleceğe bakmak yok\n",
        "    # Satır ve sütun sayısı block_size olan iki boyutlu alt taraf diagonal 1 olan bir matris oluştur : İlk satırdaki eleman sadece kendini görür-2.Satırdaki eleman bir öncekini ve kendini görür...\n",
        "    self.register_buffer('tril' , torch.tril(torch.ones(block_size,block_size)))\n",
        "    # Unutma oranı\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    #Ne oluyor? Tek bir self-attention başı tanımlıyoruz. Her biri, giriş vektörlerini (n_embd=64) daha küçük bir boyuta (head_size=16) dönüştürüyor (key, query, value için).\n",
        "    # tril, gelecekteki zaman dilimlerini maskelemek için alt üçgen matris. Dropout, overfitting’i önler (ama dropout=0).\n",
        "    #Hikaye bağlamı: Öğrencimiz, her kelimeyi/karakteri 64 boyutlu bir notla temsil ediyor ve bu notları n_embd // n_head =  16 boyutlu özetlere (key, query, value) indirgiyor. Geleceği görmemek için bir perde (tril) kullanıyor.\n",
        "\n",
        "  def forward(self,x):\n",
        "    B , T , C = x.shape\n",
        "    k = self.key(x)  # (B , T , C)\n",
        "    q = self.query(x)  # (B, T , C)\n",
        "    # Dikkat skorlarını hesapla\n",
        "    wei = q @ k.transpose(-2,-1) * C**-0.5          #B, T, C) @ (B, C, T) -> (B, T, T) ---Not:** C**-0.5 ile ölçekleme, sayıların çok büyümesini engelleyerek eğitimi stabil hale getirir.\n",
        "    wei = wei.masked_fill(self.tril[:T , :T] == 0 , float('-inf'))  # (B , T , T)\n",
        "    wei= F.softmax(wei , dim = -1)  # (B , T , T)\n",
        "    wei = self.dropout(wei)\n",
        "    # Değerleri (value) dikkat skorlarına göre ağırlıklı topla\n",
        "    v = self.value(x)  # (B , T , C)\n",
        "    out = wei @ v   # (B , T , T) @ (B , T , C) ----> (B , T , C)\n",
        "    return out"
      ],
      "metadata": {
        "id": "e0J9lxSQ3aKW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ne oluyor? Self-attention mekanizması:**\n",
        "- 1. Giriş x (B,T,64)’ü key, query, value’ye dönüştürüyor (her biri B,T,16).\n",
        "- 2. Dikkat skorları (wei) hesaplanıyor: query ve key’in nokta çarpımı, normalize edilmiş (C**-0.5 ile ölçekleme, patlamayı önler).\n",
        "- 3. Gelecek maskeleniyor (tril), softmax ile ağırlıklar normalize ediliyor.\n",
        "- 4. Value vektörleri ağırlıklı olarak birleştiriliyor.\n",
        "**Hikaye bağlamı: Öğrencimiz, her harfin diğer harflerle ilişkisini (query-key) ölçüyor, sadece geçmişe bakıyor, ağırlıkları hesaplıyor ve geçmiş harflerin notlarını (value) birleştiriyor.\n",
        "Boyut: Giriş (B,T,64), çıkış (B,T,16)."
      ],
      "metadata": {
        "id": "k69VVNy19bi1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##B.MultiHeadAttention Sınıfı --- Çoklu Dikkat,\n",
        "- Neden tek bir bakış açısıyla yetinelim?\n",
        "- Bu modül birden fazla Head 'i paralel olarak çalıştırır"
      ],
      "metadata": {
        "id": "rznSHSVp_kB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self , num_heads , head_size):\n",
        "    super().__init__()\n",
        "    # Belirtilen sayıda Head oluştur ve bir listede tut\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    # Head'lerden gelen sonuçları birleştiren bşr projeksiyon katmanı\n",
        "    self.proj = nn.Linear(n_embd , n_embd)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,x):\n",
        "    # Her bir Head sonucunu hesapla ve bunları birleştir\n",
        "    out = torch.cat([h(x) for h in self.heads] , dim = -1 )\n",
        "    # Birleştirilmiş sonucu projeksiyon katmanından geçir\n",
        "    out = self.dropout(self.proj(out))\n",
        "    return out\n",
        "\n",
        "# Hikayesi: Bir konuyu anlamak için farklı uzmanlara danışmak gibidir.\n",
        "#Bir Head kelimelerin fiil-nesne ilişkisine odaklanırken, diğeri sıfat-isim ilişkisine odaklanabilir.\n",
        "#Sonra tüm bu uzmanların görüşlerini (out) birleştirip ortak bir sonuca (proj(out)) varırız."
      ],
      "metadata": {
        "id": "J0suc_c3_eZ_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ne oluyor?**\n",
        "-  n_head=4 tane dikkat başını paralel çalıştırıyoruz. Her baş, head_size=16 (çünkü n_embd=64 / n_head=16). Sonuçlar birleştirilip proj ile tekrar n_embd=64 boyutuna getiriliyor.\n",
        "\n",
        "**Hikaye bağlamı:**\n",
        "-  Öğrencimiz, 4 farklı açıdan metni inceletiyor. Sonra hepsinin notlarını birleştirip son bir özet yazıyor.\n",
        "-  Her başın çıktısı (B,T,16) birleştiriliyor (B,T,64), sonra lineer dönüşüm ve dropout uygulanıyor.\n",
        "Boyut: Giriş (B,T,64), çıkış (B,T,64)."
      ],
      "metadata": {
        "id": "Yj9Q9duNBsbs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##C. FeedForward Sınıfı : Düşünme Modülü\n",
        "- Dikkat mekanizması kelimelerin birbiriyle iletişim kurmasını sağlar.\n",
        "- Bu katman ise her kelimenin topladığı bilgilerle tek başına düşünmesini sağlar."
      ],
      "metadata": {
        "id": "jQ31l-9CDf_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self , n_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embd , 4 * n_embd), #Gelen bilgiyi daha geniş bir alana yay --> Daha derin düşünmek için\n",
        "        nn.ReLU(),                      # Negatif bilgileri at , sadece pozitif bilgileri tut\n",
        "        nn.Linear(4 * n_embd , n_embd), # Düşünceyi tekrar orjinal boyutuna sıkıştır\n",
        "        nn.Dropout(dropout),\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.net(x)\n",
        "\n",
        "#Hikayesi: Öğrenci, diğerlerinden fısıltıyla bilgi topladıktan sonra (MultiHeadAttention),\n",
        "# sırasına çekilip bu bilgileri kendi içinde işler, analiz eder ve bir sonuca varır."
      ],
      "metadata": {
        "id": "fVmyu9WnD3t0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## D. Block Sınıfı : Bir derslik Ünite\n",
        "- Bu bir transformatörün temel yapı taşıdır.\n",
        "- Bir \"iletişim\" adımını(MultiHeadAttention) ve bir \"düşünme\" adımını(FeedForward) birleştirir."
      ],
      "metadata": {
        "id": "IpMcddHAFXNv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "  def __init__(self , n_embd , n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embd // n_head\n",
        "\n",
        "    self.sa = MultiHeadAttention(n_head , head_size) # İletişim modülü\n",
        "    self.ffwd = FeedForward(n_embd)                  # Düşünme modülü\n",
        "    self.ln1 = nn.LayerNorm(n_embd)                  # Normalizasyon katmanları\n",
        "    self.ln2  = nn.LayerNorm(n_embd)\n",
        "\n",
        "  def forward(self,x):\n",
        "    # Önceki bilgi (x) + yeni iletişimden gelen bilgi (sa). Buna \"residual connection\" denir\n",
        "    x = x + self.sa(self.ln1(x))\n",
        "    # Bir önceki adımın sonucu (x) + yeni düşünme modülünden gelen bilgi(ffwd)\n",
        "    x = x + self.ffwd(self.ln2(x))\n",
        "    return x\n",
        "\n",
        "# Hikayesi : Bir ders ünitesi gibidir.Önce öğrenciler birbiriyle konuşur(sa)\n",
        "# sonra öğrendiklerini kendi başına düşünürler(ffwd)\n",
        "# x = x + ... kısmı çok önemlidir : Bu öğrenci, \"yeni bir şey öğrenirken eski bildiklerini unutmasın , sadece üzerine eklesin\" demektir"
      ],
      "metadata": {
        "id": "uzfwFeMuFwsB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Bölüm : Beynin Tamamlanması ve Çalıştırılması\n",
        "\n",
        "- Tüm Parçaları birleştirip tam bir dil modeli oluşturuyoruz"
      ],
      "metadata": {
        "id": "GRk3JYssIEF5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BigramLanguageModel Sınıfı"
      ],
      "metadata": {
        "id": "BFm4qpFtIWfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # Her bir karakterin başlangıç anlamını tutan tablo\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size , n_embd)\n",
        "    # Her bir pozisyonun (1. , 2. , 3. sıra ...) anlamını tutan tablo\n",
        "    self.position_embedding_table = nn.Embedding(block_size , n_embd)\n",
        "    # Belirtilen sayıda Block'u (ders ünitesini) üst üste koy\n",
        "    self.blocks = nn.Sequential(*[Block(n_embd , n_head = n_head) for _ in range(n_layer)])\n",
        "    self.ln_f = nn.LayerNorm(n_embd) # Son bir Normalizasyon\n",
        "    # Son zenginleştirilmiş anlamı  : alfabedeki her harf için bir skora dönüştüren katman\n",
        "    self.lm_head = nn.Linear(n_embd , vocab_size)\n",
        "\n",
        "\n",
        "  def forward(self , idx , targets = None):\n",
        "    B , T = idx.shape\n",
        "    # Gelen sayıların (idx) karakter ve pozisyon anlamlarını tablolardan al\n",
        "    tok_emb = self.token_embedding_table(idx) # (B ,  T , C)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T , device = device))  # (T , C)\n",
        "\n",
        "    #Bir karakterin anlamı  = kendi anlamı  + pozisyonun anlamı\n",
        "    x = tok_emb + pos_emb  # (B , T , C)\n",
        "    #Veriyi tüm düşünme katmanlarından geçir\n",
        "    x = self.blocks(x)  # (B , T , C)\n",
        "    x = self.ln_f(x)   # (B , T , C)\n",
        "    # Sonucu bir sonraki karakter için tahmin skorlarına (logits) dönüştür\n",
        "    logits = self.lm_head(x)  # (B , T , vocab_size)\n",
        "\n",
        "    # Eğer hedef(doğru cevap) verilmişse , hatayı (loss) hesapla\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else :\n",
        "      B , T , C = logits.shape\n",
        "      logits = logits.view(B * T , C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits , targets)\n",
        "    return logits , loss\n",
        "\n",
        "\n",
        "  def generate(self , idx , max_new_tokens):\n",
        "    # Bu fonksiyon yeni metin üretir\n",
        "    for _ in range(max_new_tokens):\n",
        "      # Bağlamı son block_size kadar kırp\n",
        "      idx_cond = idx[: , -block_size:]\n",
        "      #Bir sonraki karakter için tahminleri al\n",
        "      logits , loss = self(idx_cond)\n",
        "      # Sadece son karakterin tahminine odaklan\n",
        "      logits = logits[: , -1 , :] # Yeni şekil : (B , C)\n",
        "      # Skorları olasılıklara çevir\n",
        "      probs = F.softmax(logits , dim = -1) #  (B , C)\n",
        "      # Bu olasıklara göre rastgele bir sonraki karakteri seç\n",
        "      idx_next = torch.multinomial(probs , num_samples = 1) # (B , 1)\n",
        "      # Seçilen karakterleri mevcut metne ekle\n",
        "      idx = torch.cat((idx , idx_next) , dim = 1) # (B , T + 1)\n",
        "    return idx"
      ],
      "metadata": {
        "id": "0Ct3eP7yIdYU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5.Bölüm : Eğitim Süreci(Okul Günü)\n",
        "- Modeli Yarattık  , şimdi onu eğitme zamanı"
      ],
      "metadata": {
        "id": "a848C6tiM6DG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BigramLanguageModel()\n",
        "m = model.to(device)   # modeli GPU'ya taşı\n",
        "\n",
        "# Bİr optimize edici oluştur.Bu modelin hatalarından ders almasını sağlar\n",
        "optimizer = torch.optim.AdamW(model.parameters() , lr = learning_rate)\n",
        "\n",
        "# Eğitim döngüsü\n",
        "for iter in range(max_iters):\n",
        "  # Arada bir sınav yap ve sonuçları yazdır\n",
        "  if iter % eval_interval == 0:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"step {iter} : train loss {losses['train'] : .4f} , val loss {losses['val'] : .4f}\")\n",
        "\n",
        "\n",
        "  #Bir derslik veri al (bir 'batch')\n",
        "  xb , yb = get_batch('train')\n",
        "\n",
        "  # modeli çalıştır ve hatayı hesapla\n",
        "  logits , loss = model(xb , yb)\n",
        "  optimizer.zero_grad(set_to_none = True) #Eski hataları temizle\n",
        "  loss.backward()                         # Yeni hatalardan kimin ne kadar sorumlu olduğunu hesapla\n",
        "  optimizer.step()              #Parametreleri (ağırlıkları) güncelle , yani öğrenmeyi gerçekleştir\n",
        "\n",
        "\n",
        "# Hikayesi : Bu döngü modelin \"okul günüdür\".\n",
        "# Sürekli olarak dersler (get_batch) alır , ne kadar hata yaptığını görür(loss).\n",
        "# Bu hatalardan ders çıkarır(loss.backward())\n",
        "# Ve kendini geliştirir(optimizer.step())\n",
        "# Arada sırada da sınava girer (estimate_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krTw0w3qNKNI",
        "outputId": "54f9a43f-3411-4baf-b146-3693fc6944f0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0 : train loss  4.2601 , val loss  4.2646\n",
            "step 100 : train loss  2.6407 , val loss  2.6446\n",
            "step 200 : train loss  2.4995 , val loss  2.5005\n",
            "step 300 : train loss  2.4084 , val loss  2.4203\n",
            "step 400 : train loss  2.3502 , val loss  2.3623\n",
            "step 500 : train loss  2.3078 , val loss  2.3188\n",
            "step 600 : train loss  2.2527 , val loss  2.2872\n",
            "step 700 : train loss  2.2054 , val loss  2.2191\n",
            "step 800 : train loss  2.1721 , val loss  2.1978\n",
            "step 900 : train loss  2.1336 , val loss  2.1531\n",
            "step 1000 : train loss  2.1000 , val loss  2.1433\n",
            "step 1100 : train loss  2.0578 , val loss  2.1181\n",
            "step 1200 : train loss  2.0477 , val loss  2.1043\n",
            "step 1300 : train loss  2.0169 , val loss  2.0757\n",
            "step 1400 : train loss  2.0027 , val loss  2.0602\n",
            "step 1500 : train loss  1.9665 , val loss  2.0372\n",
            "step 1600 : train loss  1.9371 , val loss  2.0201\n",
            "step 1700 : train loss  1.9286 , val loss  2.0081\n",
            "step 1800 : train loss  1.9166 , val loss  2.0134\n",
            "step 1900 : train loss  1.8929 , val loss  1.9894\n",
            "step 2000 : train loss  1.8738 , val loss  1.9767\n",
            "step 2100 : train loss  1.8700 , val loss  1.9750\n",
            "step 2200 : train loss  1.8569 , val loss  1.9483\n",
            "step 2300 : train loss  1.8330 , val loss  1.9550\n",
            "step 2400 : train loss  1.8250 , val loss  1.9416\n",
            "step 2500 : train loss  1.7986 , val loss  1.9354\n",
            "step 2600 : train loss  1.8129 , val loss  1.9416\n",
            "step 2700 : train loss  1.7858 , val loss  1.9326\n",
            "step 2800 : train loss  1.7983 , val loss  1.9177\n",
            "step 2900 : train loss  1.7781 , val loss  1.9194\n",
            "step 3000 : train loss  1.7637 , val loss  1.9126\n",
            "step 3100 : train loss  1.7446 , val loss  1.8950\n",
            "step 3200 : train loss  1.7418 , val loss  1.9017\n",
            "step 3300 : train loss  1.7470 , val loss  1.8871\n",
            "step 3400 : train loss  1.7410 , val loss  1.8882\n",
            "step 3500 : train loss  1.7305 , val loss  1.8771\n",
            "step 3600 : train loss  1.7263 , val loss  1.8767\n",
            "step 3700 : train loss  1.7186 , val loss  1.8826\n",
            "step 3800 : train loss  1.7040 , val loss  1.8703\n",
            "step 3900 : train loss  1.7011 , val loss  1.8722\n",
            "step 4000 : train loss  1.7042 , val loss  1.8753\n",
            "step 4100 : train loss  1.6991 , val loss  1.8658\n",
            "step 4200 : train loss  1.6988 , val loss  1.8707\n",
            "step 4300 : train loss  1.6808 , val loss  1.8653\n",
            "step 4400 : train loss  1.6888 , val loss  1.8505\n",
            "step 4500 : train loss  1.6811 , val loss  1.8388\n",
            "step 4600 : train loss  1.6713 , val loss  1.8338\n",
            "step 4700 : train loss  1.6796 , val loss  1.8400\n",
            "step 4800 : train loss  1.6549 , val loss  1.8276\n",
            "step 4900 : train loss  1.6500 , val loss  1.8325\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.Bölüm : Mezuniyet -- Metin Üretme\n",
        "- Eğitim bitti.\n",
        "- Bakalım Ne Öğrendi"
      ],
      "metadata": {
        "id": "scCsefXyPeWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Başlangıç olarak boş bir bağlam (0) veriyoruz\n",
        "context = torch.zeros((1,1) , dtype = torch.long , device = device)\n",
        "#Modelden 2000 yeni karakter üretmesini istiyoruz ve sonucu metne çevirip yazdırıyoruz\n",
        "print(decode(m.generate(context,max_new_tokens=2000)[0].tolist()))\n",
        "\n",
        "#Hikayesi: Modelimize boş bir sayfa veriyoruz ve \"Hadi,bildiklerinle bir şeyler yaz!\" diyoruz.\n",
        "# O da karakter karakter , öğrendiği metin tarzında yeni bir metin oluşturuyor."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "augrMaG-Pmi-",
        "outputId": "c785be4e-c377-4a8a-c62e-d8e034bc28c7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "But make to never. May, whose this not.\n",
            "\n",
            "Ly blear; I would there fance more: I wound cut be the divist,\n",
            "Wring arcy mistine the good, stout,\n",
            "And bluds, If both your poicy.\n",
            "\n",
            "ESCALUS Cloming,\n",
            "O wimpon, gentle of ma'gam!\n",
            "'Twout thatIe thy donot; there go dovery;\n",
            "TheIe be that what calland they arm\n",
            "That you thou abed to be jurtuants,\n",
            "Ow speak the vice no-moler of thousand Look loves,\n",
            "Give which tread well all them guards\n",
            "Inlins thus rown thing us; that for I land?\n",
            "It no be doth nor thou horsight holO\n",
            "Yet dive though long, prisons of,\n",
            "Got to be see, Clencuar; why, eor that\n",
            "sees I' dare farew, and, earful:\n",
            "'We Eve due revent to his so the suire adverys broatted lies,\n",
            "Then ailf Oreed ands in my luce,\n",
            "Deed Your falling his banight from the thesic aring.\n",
            "\n",
            "KING EDWARV:\n",
            "Doue he hath wabs them see for Edward,\n",
            "Me'ly ass' privostantay.s 'Whal is the brack and be\n",
            "At everments the hath doth I'll upon the sonce?\n",
            "\n",
            "LUCIO:\n",
            "Is mist, abutladal gentle the queen hif is oneBabestal,\n",
            "Plan day I dore.\n",
            "That I now, blood losge with all,\n",
            "Arm, heark that so shak! swords he thyself\n",
            "O years be that beengen'd, we not.\n",
            "\n",
            "ESTA:\n",
            "They, nor for the poicess, sut whith the olp make te no?\n",
            "Shaw but think sforte them hat moting heaver'd Servanned\n",
            "Is for Gong thou not hear from to thy not fame, the to service,\n",
            "Clan it this heir advess, and All they battle arms.\n",
            "\n",
            "DUKE VINE:\n",
            "If you swords, thing call to murde,\n",
            "He praying I content to not his puldiend in,\n",
            "Onethen wass this preatter findle her said.\n",
            "\n",
            "CLIFISY:\n",
            "I speak the joy; forfines proason me;\n",
            "Sir, warn as missalanual if.\n",
            "I well st? far at limfury, noble,\n",
            "he, will noh Camillo: I will you gentlemous it and,\n",
            "Where Carilf I would love. I your after\n",
            "I infit of sadving of Gong's prit his.\n",
            "A minks wonsmalve,\n",
            "The he witte thou belived vengue their and conton there turn\n",
            "O the time boys dearnown amilt\n",
            "That us mine our list, undein From allow.\n",
            "\n",
            "DUKE VERWANCESTER:\n",
            "He'st might nevert thy bedgam,\n",
            "And stunaten I pray yet,' with that live your Dive they night,\n",
            "And insweethess \n"
          ]
        }
      ]
    }
  ]
}
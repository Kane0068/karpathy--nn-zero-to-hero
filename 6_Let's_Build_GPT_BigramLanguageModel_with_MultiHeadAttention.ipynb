{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Hikayemiz: Bir SÄ±nÄ±ftaki Ã–ÄŸrencilerin FÄ±sÄ±ltÄ± Oyunu ğŸ¤«\n",
        "Haydi, bir sÄ±nÄ±f hayal edelim. Bu sÄ±nÄ±ftaki her bir Ã¶ÄŸrenci, bir cÃ¼mlenin iÃ§indeki bir kelimeyi temsil ediyor.\n",
        "\n",
        "Ã–rneÄŸin batch_size = 4: SÄ±nÄ±fta 4 tane birbirinden baÄŸÄ±msÄ±z \"oyun grubu\" var (batch).block_size = 8: Her grupta 8 Ã¶ÄŸrenci (kelime) var. Bu Ã¶ÄŸrenciler sÄ±rayla oturuyorlar. CÃ¼mlenin sÄ±rasÄ± gibi.\n",
        "n_embd = 64: Her Ã¶ÄŸrencinin kendine ait 64 farklÄ± \"Ã¶zelliÄŸi\" var. Mesela bir Ã¶ÄŸrencinin \"neÅŸeli\", \"Ã§alÄ±ÅŸkan\", \"yorgun\" gibi 64 farklÄ± Ã¶zelliÄŸi olabilir. Bu, kelimenin anlamÄ±nÄ± taÅŸÄ±yan bir vektÃ¶r.\n",
        "Kodumuzdaki x deÄŸiÅŸkeni, iÅŸte bu 4 grubun, 8'er Ã¶ÄŸrenciden oluÅŸan ve her Ã¶ÄŸrencinin 64 Ã¶zelliÄŸi olan sÄ±nÄ±f listesidir.\n",
        "\n",
        "--------------\n",
        "\n",
        "AdÄ±m 1: Herkesin ÃœÃ§ FarklÄ± Åapka TakmasÄ± (Key, Query, Value)\n",
        "SÄ±nÄ±fta bir oyun oynayacaÄŸÄ±z. Oyunun adÄ± \"AnlamÄ± ZenginleÅŸtirme\". Her Ã¶ÄŸrencinin (kelimenin) diÄŸer Ã¶ÄŸrencilere bakarak kendi anlamÄ±nÄ± daha iyi kavramasÄ± gerekiyor.\n",
        "\n",
        "Bunun iÃ§in her Ã¶ÄŸrenciye Ã¼Ã§ farklÄ± \"rol\" veya \"ÅŸapka\" veriyoruz. Bu ÅŸapkalarÄ± takÄ±nca, sahip olduklarÄ± 64 Ã¶zellik, daha kÃ¼Ã§Ã¼k ve amaca yÃ¶nelik 4 Ã¶zelliÄŸe dÃ¶nÃ¼ÅŸÃ¼yor (head_size = 4).\n",
        "\n",
        "-----------------\n",
        "Sorgu ÅapkasÄ± (Query â“): Bir Ã¶ÄŸrenci bu ÅŸapkayÄ± taktÄ±ÄŸÄ±nda, diÄŸerlerine sormak istediÄŸi soruyu hazÄ±rlar. Bu, \"Ben kiminle ilgiliyim? Bana kimin anlamÄ± lazÄ±m?\" diye baÄŸÄ±rmak gibidir.\n",
        "q = query(x) -> Her Ã¶ÄŸrenci sorgu ÅŸapkasÄ±nÄ± taktÄ± ve elinde bir \"soru kartÄ±\" (q) oluÅŸtu.\n",
        "\n",
        "Anahtar ÅapkasÄ± (Key ğŸ”‘): Bir Ã¶ÄŸrenci bu ÅŸapkayÄ± taktÄ±ÄŸÄ±nda, kendisinin ne hakkÄ±nda olduÄŸunu Ã¶zetleyen bir \"anahtar kelime\" veya \"etiket\" oluÅŸturur. Bu, \"Benim konum bu! EÄŸer benimle ilgili bir ÅŸey arÄ±yorsan, bu etikete bak!\" demek gibidir.\n",
        "k = key(x) -> Her Ã¶ÄŸrenci anahtar ÅŸapkasÄ±nÄ± taktÄ± ve boynuna bir \"etiket\" (k) astÄ±.\n",
        "\n",
        "DeÄŸer ÅapkasÄ± (Value ğŸ): Bu ÅŸapkayÄ± takan Ã¶ÄŸrenci ise, eÄŸer birisi ona ilgi gÃ¶sterirse sunacaÄŸÄ± \"bilgi paketini\" hazÄ±rlar. Bu, \"EÄŸer bana dikkat etmeye karar verirsen, sana vereceÄŸim deÄŸerli bilgi bu.\" demek gibidir.\n",
        "v = value(x) -> Her Ã¶ÄŸrenci deÄŸer ÅŸapkasÄ±nÄ± taktÄ± ve elinde bir \"hediye paketi\" (v) tutuyor.\n",
        "\n",
        "Her Ã¶ÄŸrenci, elindeki \"soru kartÄ±nÄ±\" (q) kaldÄ±rÄ±r ve sÄ±nÄ±ftaki diÄŸer tÃ¼m Ã¶ÄŸrencilerin boynundaki \"etiketlere\" (k) bakar.\n",
        "\n",
        "q @ k.transpose(...): Bu matematiksel iÅŸlem (@), bir matris Ã§arpÄ±mÄ±dÄ±r ama biz buna \"ilgi Ã¶lÃ§er\" diyelim. Her Ã¶ÄŸrencinin sorusu, diÄŸer tÃ¼m Ã¶ÄŸrencilerin etiketleriyle \"Ã§arpÄ±lÄ±r\" ve bir uyumluluk puanÄ± ortaya Ã§Ä±kar.\n",
        "EÄŸer bir Ã¶ÄŸrencinin sorusu, baÅŸka bir Ã¶ÄŸrencinin etiketiyle Ã§ok uyumluysa, aralarÄ±ndaki puan (wei, yani weight/aÄŸÄ±rlÄ±k) yÃ¼ksek olur. Uyumsuzsa, puan dÃ¼ÅŸÃ¼k olur.\n",
        "SonuÃ§ta elimizde ne var? 8x8'lik bir tablo! Bu tablo, her Ã¶ÄŸrencinin diÄŸer her bir Ã¶ÄŸrenciye (kendisi de dahil) ne kadar ilgi duyduÄŸunu gÃ¶steren bir \"ilgi haritasÄ±\"dÄ±r.\n",
        "AdÄ±m 3: GeleceÄŸi GÃ¶rmek Yasak! (Maskeleme)\n",
        "Bu oyunda Ã¶nemli bir kural var: Ã–ÄŸrenciler kendilerinden sonra gelen Ã¶ÄŸrencilerden kopya Ã§ekemez! Yani bir kelime, cÃ¼mlenin ilerisindeki bir kelimenin anlamÄ±ndan etkilenemez. Sadece kendinden Ã¶ncekilere bakabilir.\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "torch.tril(...): Bu kod, sihirli bir \"kural maskesi\" (tril) yaratÄ±r. Bu maske, ilgi haritamÄ±zla aynÄ± boyutta (8x8) ve sadece sol alt Ã¼Ã§geni 1'lerle dolu. Geri kalanÄ± 0. (1 = bakabilirsin, 0 = bakamazsÄ±n).\n",
        "masked_fill(...): Bu komutla kuralÄ± uyguluyoruz. Ä°lgi haritamÄ±zdaki (wei) bakÄ±lmasÄ± yasak olan tÃ¼m yerleri (maskenin 0 olduÄŸu yerleri) negatif sonsuz (-inf) puanÄ± ile doldururuz. Bu, \"Oraya bakman o kadar yasak ki, ihtimali bile yok!\" demektir.\n",
        "ArtÄ±k ilgi haritamÄ±z, her Ã¶ÄŸrencinin sadece kendisi ve kendinden Ã¶ncekilere olan ilgi puanlarÄ±nÄ± iÃ§eriyor.\n",
        "\n",
        "AdÄ±m 4: Ä°lgi DaÄŸÄ±lÄ±mÄ± (Softmax)\n",
        "Elimizde ham ilgi puanlarÄ± var. Ama bir Ã¶ÄŸrencinin toplamda %100'lÃ¼k bir \"dikkat\" kaynaÄŸÄ± var. Bu kaynaÄŸÄ±, ilgi duyduÄŸu kiÅŸilere nasÄ±l daÄŸÄ±tacak? Ä°ÅŸte burada Softmax devreye giriyor.\n",
        "\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "Softmax, ham ve anlamsÄ±z puanlarÄ± (Ã¶rneÄŸin: 2.3, -1.5, 10.8) alÄ±r ve onlarÄ± gÃ¼zel bir olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r. Yani, her Ã¶ÄŸrenci iÃ§in ilgi duyduÄŸu diÄŸer Ã¶ÄŸrencilere verdiÄŸi dikkat yÃ¼zdelerini hesaplar.\n",
        "\n",
        "PuanÄ± yÃ¼ksek olan Ã¶ÄŸrenci, dikkatin bÃ¼yÃ¼k bir kÄ±smÄ±nÄ± alÄ±r (%70 gibi).\n",
        "PuanÄ± dÃ¼ÅŸÃ¼k olan Ã¶ÄŸrenci, az bir dikkat alÄ±r (%5 gibi).\n",
        "PuanÄ± negatif sonsuz olan (gelecekteki Ã¶ÄŸrenciler) ise %0 dikkat alÄ±r.\n",
        "SonuÃ§ta wei tablomuz, artÄ±k her Ã¶ÄŸrencinin dikkatini geÃ§miÅŸteki hangi Ã¶ÄŸrencilere yÃ¼zde kaÃ§ oranÄ±nda daÄŸÄ±ttÄ±ÄŸÄ±nÄ± gÃ¶steren net bir \"dikkat haritasÄ±\" haline geldi.\n",
        "\n",
        "AdÄ±m 5: Bilgileri Toplama ve AnlamÄ± ZenginleÅŸtirme\n",
        "Geldik son adÄ±ma! ArtÄ±k her Ã¶ÄŸrenci kimlere ne kadar dikkat edeceÄŸini biliyor. Åimdi, o dikkat oranÄ±nda onlardan bilgi toplama zamanÄ±.\n",
        "\n",
        "out = wei @ v\n",
        "Her Ã¶ÄŸrenci, elindeki \"dikkat haritasÄ±na\" (wei) bakar.\n",
        "Diyelim ki 3. Ã¶ÄŸrenci, 1. Ã¶ÄŸrenciye %20, 2. Ã¶ÄŸrenciye ise %80 dikkat etmeye karar verdi.\n",
        "O zaman 1. Ã¶ÄŸrencinin elindeki \"hediye paketinden\" (v) %20 alÄ±r, 2. Ã¶ÄŸrencinin hediye paketinden ise %80 alÄ±r.\n",
        "Bu topladÄ±ÄŸÄ± bilgileri birleÅŸtirerek kendi yeni zenginleÅŸtirilmiÅŸ bilgisini (out) oluÅŸturur.\n",
        "Ä°ÅŸte bu out deÄŸiÅŸkeni, her Ã¶ÄŸrencinin (kelimenin) sadece kendi baÅŸÄ±na deÄŸil, cÃ¼mlenin baÄŸlamÄ±na (kendinden Ã¶nceki kelimelere) dikkat ederek oluÅŸturduÄŸu yeni ve daha derin anlamÄ±dÄ±r!"
      ],
      "metadata": {
        "id": "QsdMdOt46h_R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.BÃ¶lÃ¼m:Projenin PlanÄ± ve Malzemeler(Hyperparameters & Data)"
      ],
      "metadata": {
        "id": "s0pAA3Y_2pKZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##A.KÃ¼tÃ¼phaneleri dahil etme"
      ],
      "metadata": {
        "id": "AoSzQyGRx8MO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "qBOBLxOu7EMv"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##B.Hiperparametreler"
      ],
      "metadata": {
        "id": "BM3v-S8sx4XO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 16 # Paralel olarak kaÃ§ tane baÄŸÄ±msÄ±z metin dizisini iÅŸleyeceÄŸiz?\n",
        "block_size = 32 # Tahminler iÃ§in maximum baÄŸlam uzunluÄŸu (Ne kadar geÃ§miÅŸe bakabilir?)\n",
        "max_iters = 5000 # Toplamda kaÃ§ eÄŸitim adÄ±mÄ±(ders) yapÄ±lacak?\n",
        "eval_interval = 100 # Her 100 derste bir Ã¶ÄŸrenme durumunu kontrol et(sÄ±nav yap)\n",
        "learning_rate = 1e-3 # Ã–ÄŸrenme hÄ±zÄ±(Her hatadan sonra ne kadar bÃ¼yÃ¼k bir dÃ¼zeltme yapacak?)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' # Dersleri GPU'da mÄ±(hÄ±zlÄ±) yoksa CPU'da mÄ±(yavaÅŸ) iÅŸleyeceÄŸiz?\n",
        "eval_iters = 200 # SÄ±navda kaÃ§ soru sorulacak?\n",
        "n_embd = 64 # Her bir karakterin/kelimenin anlamÄ±nÄ± kaÃ§ sayÄ±yla temsil edeceÄŸiz?(Anlam ZenginliÄŸi)\n",
        "n_head = 4 # KaÃ§ tane 'dikkat baÅŸlÄ±ÄŸÄ±' (farklÄ± bakÄ±ÅŸ aÃ§Ä±sÄ±) olacak\n",
        "n_layer = 4 # KaÃ§ tane 'transformatÃ¶r bloÄŸu' (dÃ¼ÅŸÃ¼nme katmanÄ±) Ã¼st Ã¼ste konulacak?\n",
        "dropout = 0.0 # Ezberlemeyi Ã¶nlemek iÃ§in ne kadar 'unutkanlÄ±k' ekleyeceÄŸiz ?(% 0, yani ÅŸimdilik hiÃ§)"
      ],
      "metadata": {
        "id": "Edoy91D78X9a",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Be0dIzI5HaR",
        "outputId": "192c0ed1-0dde-479e-e232-bb128683a7de"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7e277cfd12b0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##C.Veri Setini YÃ¼kleme"
      ],
      "metadata": {
        "id": "dpDyfSW_xy0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAqU5Uz-xi9v",
        "outputId": "6ae06f81-3341-46e2-83a3-a5648bde62eb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-17 19:16:51--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: â€˜input.txtâ€™\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2025-06-17 19:16:51 (83.7 MB/s) - â€˜input.txtâ€™ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##D.Karakter Seti"
      ],
      "metadata": {
        "id": "CTgih-GzyB-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Metindeki tÃ¼m benzersiz karakterleri buluyoruz.Bu bizim 'alfabemiz'\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars) # Alfabemizde kaÃ§ karakter var ?\n",
        "\n",
        "# Karakterileri sayÄ±lara (stoi) ve sayÄ±larÄ± karakterlere Ã§eviren (itos) sÃ¶zlÃ¼kler oluÅŸturuyoruz\n",
        "# Ã‡Ã¼nkÃ¼ bilgisayar harflerle deÄŸil sayÄ±larla Ã§alÄ±ÅŸÄ±r\n",
        "stoi = {ch : i for i , ch in enumerate(chars)}\n",
        "itos = {i : ch for i , ch in enumerate(chars)}\n",
        "encode = lambda s : [stoi[c] for c in s]        # Bir metni sayÄ± dizisine Ã§evirir\n",
        "decode = lambda l : ''.join([itos[i] for i in l])     # Bir sayÄ± dizisini metne Ã§evirir\n",
        "\n",
        "\n",
        "# Veriyi EÄŸitim ve SÄ±nav(Validasyon) olarak ikiye ayÄ±rÄ±yoruz\n",
        "\n",
        "data = torch.tensor(encode(text) , dtype = torch.long)\n",
        "n = int(0.9*len(data))   # Verinin %90 'Ä± eÄŸitim , % 10 sÄ±nav iÃ§in\n",
        "train_data = data[:n]\n",
        "val_data= data[n:]\n",
        "\n"
      ],
      "metadata": {
        "id": "u_6D_yrg7ggM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.BÃ¶lÃ¼m : Ders ve SÄ±nav HazÄ±rlÄ±ÄŸÄ±"
      ],
      "metadata": {
        "id": "KcMnk_wE21h_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##A.Veri YÃ¼kleme / Ders HazÄ±rlama Fonksiyonu"
      ],
      "metadata": {
        "id": "DwF9dd4CzqRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# bu fonksiyon her ders iÃ§in metinden rastgele parÃ§a seÃ§er\n",
        "\n",
        "def get_batch(split):\n",
        "  # EÄŸitim veya sÄ±nav setinden veri seÃ§ilir\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  # Metin iÃ§inden rastgele baÅŸlangÄ±Ã§ noktalarÄ± seÃ§ilir (batch_size kadar)\n",
        "  ix = torch.randint(len(data) - block_size,(batch_size,))\n",
        "  # SeÃ§ilen noktalardan block_size uzunluÄŸunda metin parÃ§alarÄ±(x) alÄ±nÄ±r\n",
        "  x = torch.stack([data[i : i + block_size] for i in ix])\n",
        "  # Hedefler (y) , x 'in bir sonraki karakteridir.Modelden bunu tahmin etmesini isteyeceÄŸiz.\n",
        "  y = torch.stack([data[i+1: i + block_size+1] for i in ix])\n",
        "  return x , y\n",
        "\n",
        "  #Hikayesi: Ã–ÄŸrenciye bir metin parÃ§asÄ± (x) veriyoruz ve \"Bu metindeki her bir karakterden sonra hangi karakter gelir?\" diye soruyoruz.\n",
        "  # DoÄŸru cevaplar da y oluyor."
      ],
      "metadata": {
        "id": "-Vtg-eVJzwep"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##B.SÄ±nav Yapma Fonksiyonu - KayÄ±p Tahmin\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zgezjbD_1INn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bu fonksiyon , modelin ne kadar iyi Ã¶ÄŸrendiÄŸini Ã¶lÃ§er.\n",
        "\n",
        "@torch.no_grad()  # Bu blokta Ã¶ÄŸrenme(gradyan hesaplama) yapma , sadece deÄŸerlendir demek iÃ§in dekoratÃ¶rÃ¼ kullandÄ±k.\n",
        "\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval() # modeli sÄ±nav moduna al\n",
        "\n",
        "  for split in ['train','val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X , Y = get_batch(split) # Bir derslik veri al\n",
        "      logits , loss = model(X ,Y) # Modelin tahminini ve hatasÄ±nÄ± al\n",
        "      losses[k] = loss.item()     # Hatam miktarÄ±nÄ± kaydet\n",
        "    out[split] = losses.mean()    # Ortalama hatayÄ± hesapla\n",
        "  model.train()   # Modeli tekrar \"Ã–ÄŸrenme Moduna\" al\n",
        "  return out\n",
        "\n",
        "#Hikayesi: Modelin Ã¶ÄŸrenmesini geÃ§ici olarak durdururuz.\n",
        "#Ona bir sÃ¼rÃ¼ soru (eval_iters kadar) sorarÄ±z ve ortalama ne kadar hata yaptÄ±ÄŸÄ±na bakarÄ±z.\n",
        "#Bu, ezber mi yapÄ±yor yoksa gerÃ§ekten Ã¶ÄŸreniyor mu anlamamÄ±zÄ± saÄŸlar."
      ],
      "metadata": {
        "id": "k8nGhj041LqN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.BÃ¶lÃ¼m : Modelin Ä°nÅŸaasÄ± (Beynin ParÃ§alarÄ±)\n",
        "- Åimdi modelin kendisini, yani \"dijital beyni\" inÅŸa ediyoruz. Bu beyin, katman katman Ã§alÄ±ÅŸan parÃ§alardan oluÅŸur.\n",
        "\n"
      ],
      "metadata": {
        "id": "eu3qPS8x2lRS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##A.Head SÄ±nÄ±fÄ± : Tek bir Dikkat KafasÄ±\n",
        "- Bir kelimenin/karakterin, kendinden Ã¶ncekilere nasÄ±l dikkat edeceÄŸini belirler."
      ],
      "metadata": {
        "id": "VR_07SLF3a3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "  def __init__(self,head_size):\n",
        "    super().__init__()\n",
        "\n",
        "    # Query , Key , Value katmanlarÄ± : Her kelimeye/karaktere Ã¼Ã§ farklÄ± ÅŸapka takar\n",
        "\n",
        "    self.key = nn.Linear(n_embd , head_size , bias = False)\n",
        "    self.query = nn.Linear(n_embd , head_size , bias = False)\n",
        "    self.value = nn.Linear(n_embd , head_size , bias = False)\n",
        "\n",
        "    # GeleceÄŸe bakmayÄ± engelleyen maske --- Not : Ä°liÅŸkileri sadece geÃ§miÅŸe bakarak Ã¶ÄŸrenecek.GeleceÄŸe bakmak yok\n",
        "    # SatÄ±r ve sÃ¼tun sayÄ±sÄ± block_size olan iki boyutlu alt taraf diagonal 1 olan bir matris oluÅŸtur : Ä°lk satÄ±rdaki eleman sadece kendini gÃ¶rÃ¼r-2.SatÄ±rdaki eleman bir Ã¶ncekini ve kendini gÃ¶rÃ¼r...\n",
        "    self.register_buffer('tril' , torch.tril(torch.ones(block_size,block_size)))\n",
        "    # Unutma oranÄ±\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    #Ne oluyor? Tek bir self-attention baÅŸÄ± tanÄ±mlÄ±yoruz. Her biri, giriÅŸ vektÃ¶rlerini (n_embd=64) daha kÃ¼Ã§Ã¼k bir boyuta (head_size=16) dÃ¶nÃ¼ÅŸtÃ¼rÃ¼yor (key, query, value iÃ§in).\n",
        "    # tril, gelecekteki zaman dilimlerini maskelemek iÃ§in alt Ã¼Ã§gen matris. Dropout, overfittingâ€™i Ã¶nler (ama dropout=0).\n",
        "    #Hikaye baÄŸlamÄ±: Ã–ÄŸrencimiz, her kelimeyi/karakteri 64 boyutlu bir notla temsil ediyor ve bu notlarÄ± n_embd // n_head =  16 boyutlu Ã¶zetlere (key, query, value) indirgiyor. GeleceÄŸi gÃ¶rmemek iÃ§in bir perde (tril) kullanÄ±yor.\n",
        "\n",
        "  def forward(self,x):\n",
        "    B , T , C = x.shape\n",
        "    k = self.key(x)  # (B , T , C)\n",
        "    q = self.query(x)  # (B, T , C)\n",
        "    # Dikkat skorlarÄ±nÄ± hesapla\n",
        "    wei = q @ k.transpose(-2,-1) * C**-0.5          #B, T, C) @ (B, C, T) -> (B, T, T) ---Not:** C**-0.5 ile Ã¶lÃ§ekleme, sayÄ±larÄ±n Ã§ok bÃ¼yÃ¼mesini engelleyerek eÄŸitimi stabil hale getirir.\n",
        "    wei = wei.masked_fill(self.tril[:T , :T] == 0 , float('-inf'))  # (B , T , T)\n",
        "    wei= F.softmax(wei , dim = -1)  # (B , T , T)\n",
        "    wei = self.dropout(wei)\n",
        "    # DeÄŸerleri (value) dikkat skorlarÄ±na gÃ¶re aÄŸÄ±rlÄ±klÄ± topla\n",
        "    v = self.value(x)  # (B , T , C)\n",
        "    out = wei @ v   # (B , T , T) @ (B , T , C) ----> (B , T , C)\n",
        "    return out"
      ],
      "metadata": {
        "id": "e0J9lxSQ3aKW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ne oluyor? Self-attention mekanizmasÄ±:**\n",
        "- 1. GiriÅŸ x (B,T,64)â€™Ã¼ key, query, valueâ€™ye dÃ¶nÃ¼ÅŸtÃ¼rÃ¼yor (her biri B,T,16).\n",
        "- 2. Dikkat skorlarÄ± (wei) hesaplanÄ±yor: query ve keyâ€™in nokta Ã§arpÄ±mÄ±, normalize edilmiÅŸ (C**-0.5 ile Ã¶lÃ§ekleme, patlamayÄ± Ã¶nler).\n",
        "- 3. Gelecek maskeleniyor (tril), softmax ile aÄŸÄ±rlÄ±klar normalize ediliyor.\n",
        "- 4. Value vektÃ¶rleri aÄŸÄ±rlÄ±klÄ± olarak birleÅŸtiriliyor.\n",
        "**Hikaye baÄŸlamÄ±: Ã–ÄŸrencimiz, her harfin diÄŸer harflerle iliÅŸkisini (query-key) Ã¶lÃ§Ã¼yor, sadece geÃ§miÅŸe bakÄ±yor, aÄŸÄ±rlÄ±klarÄ± hesaplÄ±yor ve geÃ§miÅŸ harflerin notlarÄ±nÄ± (value) birleÅŸtiriyor.\n",
        "Boyut: GiriÅŸ (B,T,64), Ã§Ä±kÄ±ÅŸ (B,T,16)."
      ],
      "metadata": {
        "id": "k69VVNy19bi1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##B.MultiHeadAttention SÄ±nÄ±fÄ± --- Ã‡oklu Dikkat,\n",
        "- Neden tek bir bakÄ±ÅŸ aÃ§Ä±sÄ±yla yetinelim?\n",
        "- Bu modÃ¼l birden fazla Head 'i paralel olarak Ã§alÄ±ÅŸtÄ±rÄ±r"
      ],
      "metadata": {
        "id": "rznSHSVp_kB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self , num_heads , head_size):\n",
        "    super().__init__()\n",
        "    # Belirtilen sayÄ±da Head oluÅŸtur ve bir listede tut\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    # Head'lerden gelen sonuÃ§larÄ± birleÅŸtiren bÅŸr projeksiyon katmanÄ±\n",
        "    self.proj = nn.Linear(n_embd , n_embd)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,x):\n",
        "    # Her bir Head sonucunu hesapla ve bunlarÄ± birleÅŸtir\n",
        "    out = torch.cat([h(x) for h in self.heads] , dim = -1 )\n",
        "    # BirleÅŸtirilmiÅŸ sonucu projeksiyon katmanÄ±ndan geÃ§ir\n",
        "    out = self.dropout(self.proj(out))\n",
        "    return out\n",
        "\n",
        "# Hikayesi: Bir konuyu anlamak iÃ§in farklÄ± uzmanlara danÄ±ÅŸmak gibidir.\n",
        "#Bir Head kelimelerin fiil-nesne iliÅŸkisine odaklanÄ±rken, diÄŸeri sÄ±fat-isim iliÅŸkisine odaklanabilir.\n",
        "#Sonra tÃ¼m bu uzmanlarÄ±n gÃ¶rÃ¼ÅŸlerini (out) birleÅŸtirip ortak bir sonuca (proj(out)) varÄ±rÄ±z."
      ],
      "metadata": {
        "id": "J0suc_c3_eZ_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ne oluyor?**\n",
        "-  n_head=4 tane dikkat baÅŸÄ±nÄ± paralel Ã§alÄ±ÅŸtÄ±rÄ±yoruz. Her baÅŸ, head_size=16 (Ã§Ã¼nkÃ¼ n_embd=64 / n_head=16). SonuÃ§lar birleÅŸtirilip proj ile tekrar n_embd=64 boyutuna getiriliyor.\n",
        "\n",
        "**Hikaye baÄŸlamÄ±:**\n",
        "-  Ã–ÄŸrencimiz, 4 farklÄ± aÃ§Ä±dan metni inceletiyor. Sonra hepsinin notlarÄ±nÄ± birleÅŸtirip son bir Ã¶zet yazÄ±yor.\n",
        "-  Her baÅŸÄ±n Ã§Ä±ktÄ±sÄ± (B,T,16) birleÅŸtiriliyor (B,T,64), sonra lineer dÃ¶nÃ¼ÅŸÃ¼m ve dropout uygulanÄ±yor.\n",
        "Boyut: GiriÅŸ (B,T,64), Ã§Ä±kÄ±ÅŸ (B,T,64)."
      ],
      "metadata": {
        "id": "Yj9Q9duNBsbs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##C. FeedForward SÄ±nÄ±fÄ± : DÃ¼ÅŸÃ¼nme ModÃ¼lÃ¼\n",
        "- Dikkat mekanizmasÄ± kelimelerin birbiriyle iletiÅŸim kurmasÄ±nÄ± saÄŸlar.\n",
        "- Bu katman ise her kelimenin topladÄ±ÄŸÄ± bilgilerle tek baÅŸÄ±na dÃ¼ÅŸÃ¼nmesini saÄŸlar."
      ],
      "metadata": {
        "id": "jQ31l-9CDf_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self , n_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embd , 4 * n_embd), #Gelen bilgiyi daha geniÅŸ bir alana yay --> Daha derin dÃ¼ÅŸÃ¼nmek iÃ§in\n",
        "        nn.ReLU(),                      # Negatif bilgileri at , sadece pozitif bilgileri tut\n",
        "        nn.Linear(4 * n_embd , n_embd), # DÃ¼ÅŸÃ¼nceyi tekrar orjinal boyutuna sÄ±kÄ±ÅŸtÄ±r\n",
        "        nn.Dropout(dropout),\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.net(x)\n",
        "\n",
        "#Hikayesi: Ã–ÄŸrenci, diÄŸerlerinden fÄ±sÄ±ltÄ±yla bilgi topladÄ±ktan sonra (MultiHeadAttention),\n",
        "# sÄ±rasÄ±na Ã§ekilip bu bilgileri kendi iÃ§inde iÅŸler, analiz eder ve bir sonuca varÄ±r."
      ],
      "metadata": {
        "id": "fVmyu9WnD3t0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## D. Block SÄ±nÄ±fÄ± : Bir derslik Ãœnite\n",
        "- Bu bir transformatÃ¶rÃ¼n temel yapÄ± taÅŸÄ±dÄ±r.\n",
        "- Bir \"iletiÅŸim\" adÄ±mÄ±nÄ±(MultiHeadAttention) ve bir \"dÃ¼ÅŸÃ¼nme\" adÄ±mÄ±nÄ±(FeedForward) birleÅŸtirir."
      ],
      "metadata": {
        "id": "IpMcddHAFXNv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "  def __init__(self , n_embd , n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embd // n_head\n",
        "\n",
        "    self.sa = MultiHeadAttention(n_head , head_size) # Ä°letiÅŸim modÃ¼lÃ¼\n",
        "    self.ffwd = FeedForward(n_embd)                  # DÃ¼ÅŸÃ¼nme modÃ¼lÃ¼\n",
        "    self.ln1 = nn.LayerNorm(n_embd)                  # Normalizasyon katmanlarÄ±\n",
        "    self.ln2  = nn.LayerNorm(n_embd)\n",
        "\n",
        "  def forward(self,x):\n",
        "    # Ã–nceki bilgi (x) + yeni iletiÅŸimden gelen bilgi (sa). Buna \"residual connection\" denir\n",
        "    x = x + self.sa(self.ln1(x))\n",
        "    # Bir Ã¶nceki adÄ±mÄ±n sonucu (x) + yeni dÃ¼ÅŸÃ¼nme modÃ¼lÃ¼nden gelen bilgi(ffwd)\n",
        "    x = x + self.ffwd(self.ln2(x))\n",
        "    return x\n",
        "\n",
        "# Hikayesi : Bir ders Ã¼nitesi gibidir.Ã–nce Ã¶ÄŸrenciler birbiriyle konuÅŸur(sa)\n",
        "# sonra Ã¶ÄŸrendiklerini kendi baÅŸÄ±na dÃ¼ÅŸÃ¼nÃ¼rler(ffwd)\n",
        "# x = x + ... kÄ±smÄ± Ã§ok Ã¶nemlidir : Bu Ã¶ÄŸrenci, \"yeni bir ÅŸey Ã¶ÄŸrenirken eski bildiklerini unutmasÄ±n , sadece Ã¼zerine eklesin\" demektir"
      ],
      "metadata": {
        "id": "uzfwFeMuFwsB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. BÃ¶lÃ¼m : Beynin TamamlanmasÄ± ve Ã‡alÄ±ÅŸtÄ±rÄ±lmasÄ±\n",
        "\n",
        "- TÃ¼m ParÃ§alarÄ± birleÅŸtirip tam bir dil modeli oluÅŸturuyoruz"
      ],
      "metadata": {
        "id": "GRk3JYssIEF5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BigramLanguageModel SÄ±nÄ±fÄ±"
      ],
      "metadata": {
        "id": "BFm4qpFtIWfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # Her bir karakterin baÅŸlangÄ±Ã§ anlamÄ±nÄ± tutan tablo\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size , n_embd)\n",
        "    # Her bir pozisyonun (1. , 2. , 3. sÄ±ra ...) anlamÄ±nÄ± tutan tablo\n",
        "    self.position_embedding_table = nn.Embedding(block_size , n_embd)\n",
        "    # Belirtilen sayÄ±da Block'u (ders Ã¼nitesini) Ã¼st Ã¼ste koy\n",
        "    self.blocks = nn.Sequential(*[Block(n_embd , n_head = n_head) for _ in range(n_layer)])\n",
        "    self.ln_f = nn.LayerNorm(n_embd) # Son bir Normalizasyon\n",
        "    # Son zenginleÅŸtirilmiÅŸ anlamÄ±  : alfabedeki her harf iÃ§in bir skora dÃ¶nÃ¼ÅŸtÃ¼ren katman\n",
        "    self.lm_head = nn.Linear(n_embd , vocab_size)\n",
        "\n",
        "\n",
        "  def forward(self , idx , targets = None):\n",
        "    B , T = idx.shape\n",
        "    # Gelen sayÄ±larÄ±n (idx) karakter ve pozisyon anlamlarÄ±nÄ± tablolardan al\n",
        "    tok_emb = self.token_embedding_table(idx) # (B ,  T , C)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T , device = device))  # (T , C)\n",
        "\n",
        "    #Bir karakterin anlamÄ±  = kendi anlamÄ±  + pozisyonun anlamÄ±\n",
        "    x = tok_emb + pos_emb  # (B , T , C)\n",
        "    #Veriyi tÃ¼m dÃ¼ÅŸÃ¼nme katmanlarÄ±ndan geÃ§ir\n",
        "    x = self.blocks(x)  # (B , T , C)\n",
        "    x = self.ln_f(x)   # (B , T , C)\n",
        "    # Sonucu bir sonraki karakter iÃ§in tahmin skorlarÄ±na (logits) dÃ¶nÃ¼ÅŸtÃ¼r\n",
        "    logits = self.lm_head(x)  # (B , T , vocab_size)\n",
        "\n",
        "    # EÄŸer hedef(doÄŸru cevap) verilmiÅŸse , hatayÄ± (loss) hesapla\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else :\n",
        "      B , T , C = logits.shape\n",
        "      logits = logits.view(B * T , C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits , targets)\n",
        "    return logits , loss\n",
        "\n",
        "\n",
        "  def generate(self , idx , max_new_tokens):\n",
        "    # Bu fonksiyon yeni metin Ã¼retir\n",
        "    for _ in range(max_new_tokens):\n",
        "      # BaÄŸlamÄ± son block_size kadar kÄ±rp\n",
        "      idx_cond = idx[: , -block_size:]\n",
        "      #Bir sonraki karakter iÃ§in tahminleri al\n",
        "      logits , loss = self(idx_cond)\n",
        "      # Sadece son karakterin tahminine odaklan\n",
        "      logits = logits[: , -1 , :] # Yeni ÅŸekil : (B , C)\n",
        "      # SkorlarÄ± olasÄ±lÄ±klara Ã§evir\n",
        "      probs = F.softmax(logits , dim = -1) #  (B , C)\n",
        "      # Bu olasÄ±klara gÃ¶re rastgele bir sonraki karakteri seÃ§\n",
        "      idx_next = torch.multinomial(probs , num_samples = 1) # (B , 1)\n",
        "      # SeÃ§ilen karakterleri mevcut metne ekle\n",
        "      idx = torch.cat((idx , idx_next) , dim = 1) # (B , T + 1)\n",
        "    return idx"
      ],
      "metadata": {
        "id": "0Ct3eP7yIdYU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5.BÃ¶lÃ¼m : EÄŸitim SÃ¼reci(Okul GÃ¼nÃ¼)\n",
        "- Modeli YarattÄ±k  , ÅŸimdi onu eÄŸitme zamanÄ±"
      ],
      "metadata": {
        "id": "a848C6tiM6DG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BigramLanguageModel()\n",
        "m = model.to(device)   # modeli GPU'ya taÅŸÄ±\n",
        "\n",
        "# BÄ°r optimize edici oluÅŸtur.Bu modelin hatalarÄ±ndan ders almasÄ±nÄ± saÄŸlar\n",
        "optimizer = torch.optim.AdamW(model.parameters() , lr = learning_rate)\n",
        "\n",
        "# EÄŸitim dÃ¶ngÃ¼sÃ¼\n",
        "for iter in range(max_iters):\n",
        "  # Arada bir sÄ±nav yap ve sonuÃ§larÄ± yazdÄ±r\n",
        "  if iter % eval_interval == 0:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"step {iter} : train loss {losses['train'] : .4f} , val loss {losses['val'] : .4f}\")\n",
        "\n",
        "\n",
        "  #Bir derslik veri al (bir 'batch')\n",
        "  xb , yb = get_batch('train')\n",
        "\n",
        "  # modeli Ã§alÄ±ÅŸtÄ±r ve hatayÄ± hesapla\n",
        "  logits , loss = model(xb , yb)\n",
        "  optimizer.zero_grad(set_to_none = True) #Eski hatalarÄ± temizle\n",
        "  loss.backward()                         # Yeni hatalardan kimin ne kadar sorumlu olduÄŸunu hesapla\n",
        "  optimizer.step()              #Parametreleri (aÄŸÄ±rlÄ±klarÄ±) gÃ¼ncelle , yani Ã¶ÄŸrenmeyi gerÃ§ekleÅŸtir\n",
        "\n",
        "\n",
        "# Hikayesi : Bu dÃ¶ngÃ¼ modelin \"okul gÃ¼nÃ¼dÃ¼r\".\n",
        "# SÃ¼rekli olarak dersler (get_batch) alÄ±r , ne kadar hata yaptÄ±ÄŸÄ±nÄ± gÃ¶rÃ¼r(loss).\n",
        "# Bu hatalardan ders Ã§Ä±karÄ±r(loss.backward())\n",
        "# Ve kendini geliÅŸtirir(optimizer.step())\n",
        "# Arada sÄ±rada da sÄ±nava girer (estimate_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krTw0w3qNKNI",
        "outputId": "54f9a43f-3411-4baf-b146-3693fc6944f0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0 : train loss  4.2601 , val loss  4.2646\n",
            "step 100 : train loss  2.6407 , val loss  2.6446\n",
            "step 200 : train loss  2.4995 , val loss  2.5005\n",
            "step 300 : train loss  2.4084 , val loss  2.4203\n",
            "step 400 : train loss  2.3502 , val loss  2.3623\n",
            "step 500 : train loss  2.3078 , val loss  2.3188\n",
            "step 600 : train loss  2.2527 , val loss  2.2872\n",
            "step 700 : train loss  2.2054 , val loss  2.2191\n",
            "step 800 : train loss  2.1721 , val loss  2.1978\n",
            "step 900 : train loss  2.1336 , val loss  2.1531\n",
            "step 1000 : train loss  2.1000 , val loss  2.1433\n",
            "step 1100 : train loss  2.0578 , val loss  2.1181\n",
            "step 1200 : train loss  2.0477 , val loss  2.1043\n",
            "step 1300 : train loss  2.0169 , val loss  2.0757\n",
            "step 1400 : train loss  2.0027 , val loss  2.0602\n",
            "step 1500 : train loss  1.9665 , val loss  2.0372\n",
            "step 1600 : train loss  1.9371 , val loss  2.0201\n",
            "step 1700 : train loss  1.9286 , val loss  2.0081\n",
            "step 1800 : train loss  1.9166 , val loss  2.0134\n",
            "step 1900 : train loss  1.8929 , val loss  1.9894\n",
            "step 2000 : train loss  1.8738 , val loss  1.9767\n",
            "step 2100 : train loss  1.8700 , val loss  1.9750\n",
            "step 2200 : train loss  1.8569 , val loss  1.9483\n",
            "step 2300 : train loss  1.8330 , val loss  1.9550\n",
            "step 2400 : train loss  1.8250 , val loss  1.9416\n",
            "step 2500 : train loss  1.7986 , val loss  1.9354\n",
            "step 2600 : train loss  1.8129 , val loss  1.9416\n",
            "step 2700 : train loss  1.7858 , val loss  1.9326\n",
            "step 2800 : train loss  1.7983 , val loss  1.9177\n",
            "step 2900 : train loss  1.7781 , val loss  1.9194\n",
            "step 3000 : train loss  1.7637 , val loss  1.9126\n",
            "step 3100 : train loss  1.7446 , val loss  1.8950\n",
            "step 3200 : train loss  1.7418 , val loss  1.9017\n",
            "step 3300 : train loss  1.7470 , val loss  1.8871\n",
            "step 3400 : train loss  1.7410 , val loss  1.8882\n",
            "step 3500 : train loss  1.7305 , val loss  1.8771\n",
            "step 3600 : train loss  1.7263 , val loss  1.8767\n",
            "step 3700 : train loss  1.7186 , val loss  1.8826\n",
            "step 3800 : train loss  1.7040 , val loss  1.8703\n",
            "step 3900 : train loss  1.7011 , val loss  1.8722\n",
            "step 4000 : train loss  1.7042 , val loss  1.8753\n",
            "step 4100 : train loss  1.6991 , val loss  1.8658\n",
            "step 4200 : train loss  1.6988 , val loss  1.8707\n",
            "step 4300 : train loss  1.6808 , val loss  1.8653\n",
            "step 4400 : train loss  1.6888 , val loss  1.8505\n",
            "step 4500 : train loss  1.6811 , val loss  1.8388\n",
            "step 4600 : train loss  1.6713 , val loss  1.8338\n",
            "step 4700 : train loss  1.6796 , val loss  1.8400\n",
            "step 4800 : train loss  1.6549 , val loss  1.8276\n",
            "step 4900 : train loss  1.6500 , val loss  1.8325\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.BÃ¶lÃ¼m : Mezuniyet -- Metin Ãœretme\n",
        "- EÄŸitim bitti.\n",
        "- BakalÄ±m Ne Ã–ÄŸrendi"
      ],
      "metadata": {
        "id": "scCsefXyPeWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BaÅŸlangÄ±Ã§ olarak boÅŸ bir baÄŸlam (0) veriyoruz\n",
        "context = torch.zeros((1,1) , dtype = torch.long , device = device)\n",
        "#Modelden 2000 yeni karakter Ã¼retmesini istiyoruz ve sonucu metne Ã§evirip yazdÄ±rÄ±yoruz\n",
        "print(decode(m.generate(context,max_new_tokens=2000)[0].tolist()))\n",
        "\n",
        "#Hikayesi: Modelimize boÅŸ bir sayfa veriyoruz ve \"Hadi,bildiklerinle bir ÅŸeyler yaz!\" diyoruz.\n",
        "# O da karakter karakter , Ã¶ÄŸrendiÄŸi metin tarzÄ±nda yeni bir metin oluÅŸturuyor."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "augrMaG-Pmi-",
        "outputId": "c785be4e-c377-4a8a-c62e-d8e034bc28c7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "But make to never. May, whose this not.\n",
            "\n",
            "Ly blear; I would there fance more: I wound cut be the divist,\n",
            "Wring arcy mistine the good, stout,\n",
            "And bluds, If both your poicy.\n",
            "\n",
            "ESCALUS Cloming,\n",
            "O wimpon, gentle of ma'gam!\n",
            "'Twout thatIe thy donot; there go dovery;\n",
            "TheIe be that what calland they arm\n",
            "That you thou abed to be jurtuants,\n",
            "Ow speak the vice no-moler of thousand Look loves,\n",
            "Give which tread well all them guards\n",
            "Inlins thus rown thing us; that for I land?\n",
            "It no be doth nor thou horsight holO\n",
            "Yet dive though long, prisons of,\n",
            "Got to be see, Clencuar; why, eor that\n",
            "sees I' dare farew, and, earful:\n",
            "'We Eve due revent to his so the suire adverys broatted lies,\n",
            "Then ailf Oreed ands in my luce,\n",
            "Deed Your falling his banight from the thesic aring.\n",
            "\n",
            "KING EDWARV:\n",
            "Doue he hath wabs them see for Edward,\n",
            "Me'ly ass' privostantay.s 'Whal is the brack and be\n",
            "At everments the hath doth I'll upon the sonce?\n",
            "\n",
            "LUCIO:\n",
            "Is mist, abutladal gentle the queen hif is oneBabestal,\n",
            "Plan day I dore.\n",
            "That I now, blood losge with all,\n",
            "Arm, heark that so shak! swords he thyself\n",
            "O years be that beengen'd, we not.\n",
            "\n",
            "ESTA:\n",
            "They, nor for the poicess, sut whith the olp make te no?\n",
            "Shaw but think sforte them hat moting heaver'd Servanned\n",
            "Is for Gong thou not hear from to thy not fame, the to service,\n",
            "Clan it this heir advess, and All they battle arms.\n",
            "\n",
            "DUKE VINE:\n",
            "If you swords, thing call to murde,\n",
            "He praying I content to not his puldiend in,\n",
            "Onethen wass this preatter findle her said.\n",
            "\n",
            "CLIFISY:\n",
            "I speak the joy; forfines proason me;\n",
            "Sir, warn as missalanual if.\n",
            "I well st? far at limfury, noble,\n",
            "he, will noh Camillo: I will you gentlemous it and,\n",
            "Where Carilf I would love. I your after\n",
            "I infit of sadving of Gong's prit his.\n",
            "A minks wonsmalve,\n",
            "The he witte thou belived vengue their and conton there turn\n",
            "O the time boys dearnown amilt\n",
            "That us mine our list, undein From allow.\n",
            "\n",
            "DUKE VERWANCESTER:\n",
            "He'st might nevert thy bedgam,\n",
            "And stunaten I pray yet,' with that live your Dive they night,\n",
            "And insweethess \n"
          ]
        }
      ]
    }
  ]
}
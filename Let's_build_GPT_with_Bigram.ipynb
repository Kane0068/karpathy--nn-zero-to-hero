{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7DOKFeJgvx64qjbsOC9vd"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z12jWLzahK58"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 32  # bir seferde paralel işlenecek bağımsız dizi sayısı\n",
        "block_size = 8   # tahminler için maksimum bağlam uzunluğu\n",
        "max_iters= 3000   # eğitim için maximum iterasyon sayısı\n",
        "eval_interval = 300  # Değerlendirme aralığı\n",
        "learning_rate = 1e-2  # Öğrenme oranı\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' # cihaz gpu varsa cuda yoksa cpu da çalışacak\n",
        "eval_iters = 200 # değerlendirme için iterasyon sayısı"
      ],
      "metadata": {
        "id": "8env7SG8hh54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- batch_size: Eğitim sırasında aynı anda işlenecek örnek sayısı. 32, her iterasyonda 32 farklı metin parçasının paralel olarak işleneceği anlamına gelir. Bu, eğitimi hızlandırır ve modelin genelleme yapmasına yardımcı olur.\n",
        "- block_size: Modelin bir seferde görebileceği maksimum metin uzunluğu (karakter cinsinden). Burada 8 karakterlik bir bağlam kullanılıyor. Örneğin, model \"Merhaba\" kelimesini tahmin ederken en fazla 8 karakter geriye bakabilir.\n",
        "- max_iters: Modelin kaç kez eğitileceği (toplam iterasyon sayısı). 3000 iterasyon boyunca model, veriyi tekrar tekrar işleyecek.\n",
        "- eval_interval: Modelin performansını değerlendirmek için kaç iterasyonda bir kontrol yapılacağı. Her 300 iterasyonda bir, modelin doğruluğu kontrol edilir.\n",
        "- learning_rate: Modelin ağırlıklarını ne kadar hızlı güncelleyeceği. 0.01 (1e-2), oldukça standart bir öğrenme oranıdır.\n",
        "- device: Eğitimde kullanılacak donanım. Eğer GPU (CUDA) varsa, eğitim hızlanır; yoksa CPU kullanılır.\n",
        "- eval_iters: Değerlendirme sırasında kaç iterasyon yapılacağı. 200 iterasyon, modelin doğruluğunu test etmek için yeterli bir örnek sağlar."
      ],
      "metadata": {
        "id": "ajrSqIOa-wnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337) # rastgelelik kontrolü. Model her çalıştırıldığın aynı rastgele sayıları üretir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AT3c98COh63G",
        "outputId": "a03562be-4987-4a85-faf6-195e2a700b99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7c0594b25170>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Veri setini indirme ve okuma\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt','r',encoding = 'utf-8') as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7B0vnDAh-W1",
        "outputId": "1069d211-dbc8-4c1f-908c-0e26f9aab685"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-12 09:48:58--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-06-12 09:48:58 (17.7 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Length of dataset in characters',len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phmq4S5kiLaj",
        "outputId": "bd9567fa-227f-4576-ee78-a50d191afdcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of dataset in characters 1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezgOlNkoiwLq",
        "outputId": "7e6f4434-3b63-42a7-ed40-a00384d2a6db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Metindeki tüm benzersiz karakterler\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# Karakterden sayıya ve sayıdan karaktere eşleştirme\n",
        "stoi = {ch : i for i,ch in enumerate(chars)}\n",
        "itos = {i : ch for i , ch in enumerate(chars)}\n",
        "encode = lambda s : [stoi[c] for c in s]   # encoder : take a string , output a list of integer ---- Stringi al karakter listesindeki her stringe karşılık gelen sayılara dönüştür\n",
        "decode = lambda l : ''.join([itos[i] for i in l]) # decoder: take a list of integers , output a string ---> İntegerların oluşturduğu listeleri al ve her integera karşılık gelen harfe dönüştür\n",
        "print('Benzersiz karakter sayısı : ',vocab_size)\n",
        "print(stoi)\n",
        "print(itos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lLwgtXvi6_B",
        "outputId": "a0cc1742-a8f8-457b-ea7e-5e95b86ccfeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Benzersiz karakter sayısı :  65\n",
            "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n",
            "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Benzersiz karakterler: Metindeki tüm farklı karakterler (a, b, ,, ., boşluk, vb.) bulunur ve sıralanır. set(text) ile tekrar eden karakterler kaldırılır, sorted() ile alfabetik sıraya dizilir\n",
        "- vocab_size: Metindeki benzersiz karakter sayısı (örneğin, 65 olabilir, çünkü İngilizce metinlerde genellikle harfler, noktalama işaretleri ve birkaç özel karakter bulunur)"
      ],
      "metadata": {
        "id": "zcbaa8vq_jh-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- stoi (String to Integer): Her karaktere bir tamsayı atar. Örneğin, 'a' → 0, 'b' → 1, ' ' → 2 gibi. Bu, metni sayısal forma çevirmek için kullanılır.\n",
        "- itos (Integer to String): Tamsayıları tekrar karaktere çevirir. Örneğin, 0 → 'a', 1 → 'b'.\n",
        "- encode: Bir metni (string) alır ve her karakteri stoi kullanarak bir tamsayı listesine çevirir. Örneğin, \"hi\" → [1, 2]).\n",
        "- decode: Bir tamsayı listesini decode kullanarak tekrar metne çevirir (örneğin, [1, 2] → \"hi\"`)."
      ],
      "metadata": {
        "id": "RNj3phhU_y9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train and test split\n",
        "data = torch.tensor(encode(text) , dtype = torch.long) # Metin verisini sayısal forma dönüştürme\n",
        "n = int(0.9*len(data))  # datasetin %90 eğitim için kullanılacak\n",
        "train_data = data[:n]  # Metnin %90'ı eğitim verisi\n",
        "val_data = data[n:]   #Metnin %10 'u doğrulama verisi"
      ],
      "metadata": {
        "id": "-E_tMYOmkkq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Veri yığını (batch) oluşturmak için fonksiyon\n",
        "def get_batch(split):\n",
        "    # Hangi veri setini kullanacağımızı seçiyoruz: eğitim('train) veya doğrulama ('val)\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    # Rastgele başlangıç indeksleri seçiyoruz(batch size kadar , veri uzunluğundan block_size çıkararak)\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    # Giriş verisi (x) : Her bir başlangıç indeksinden block_size kadar karakter alıyoruz\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    # Hedef verisi (y) : x 'in bir karakter kaydırılmış hali (sonraki karakteri tahmin edeceğiz)\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    # Verileri seçilen cihaza (GPU veya CPU ya) yaşıyoruz\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    # Giriş (x) ve hedef (y) tensorlarını döndürüyoruz\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "cwtjpqBjlARn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Amaç: Eğitim veya doğrulama için rastgele bir veri yığını (batch) üretir. Her bir batch, batch_size kadar örnek içerir ve her örnek block_size uzunluğundadır.\n",
        "- split: Hangi veri setinin kullanılacağını belirtir ('train' veya 'val').\n",
        "- ix: Veri setinden rastgele başlangıç indeksleri seçer. len(data) - block_size ile veri sonuna taşmamak için sınırlandırılır.\n",
        "- x: Giriş verisi, her biri block_size uzunluğunda tamsayı dizileridir (örneğin, \"hello\"nun kodlanmış hali: [0, 1, 2, 2, 3]).\n",
        "- y: Hedef verisi, xin bir karakter kaydırılmış halidir. Örneğin, x = [0, 1, 2, 2, 3] ise, y = [1, 2, 2, 3, ?] olur (model bir sonraki karakteri tahmin eder).\n",
        "- to(device): Veriler, GPU (cuda) veya CPU'ya taşınır, böylece modelin çalışacağı donanıma uygun olur.\n",
        "- Dönen değer: x (giriş) ve y (hedef) tensorları, şekilleri (batch_size, block_size)."
      ],
      "metadata": {
        "id": "oMfrJB7eC1Tg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modelin kaybını (loss) değerlendirmek için fonksiyon (gradyan hesaplaması olmadan)\n",
        "@torch.no_grad()  # gradyan izlemeyi durdur\n",
        "\n",
        "def estimate_loss(): # Tahmini kayıp\n",
        "  #Sonuçları saklamak için bir sözlük\n",
        "  out = {}\n",
        "  # modeli değerlendirme moduna geçir\n",
        "  model.eval()\n",
        "  # Eğitim ve doğrulama setleri için bir döngü başlatılır\n",
        "  for split in ['train','val']:\n",
        "    # eval_iters(200) kadar kayıp değerini saklamak için sıfırlardan oluşan bşr tensor\n",
        "    losses = torch.zeros(eval_iters)\n",
        "\n",
        "    # eval iters kadar döngü\n",
        "    for k in range(eval_iters):\n",
        "      # Rastgele bir veri yığını al\n",
        "      X , Y = get_batch(split)\n",
        "      # modeli çalıştırarak logits ve kaybı hesapla\n",
        "      logits , loss = model(X,Y)\n",
        "      # kaybı kaydet\n",
        "      losses[k] = loss.item()\n",
        "    # Ortalama kaybı sözlüğe ekle\n",
        "    out[split] = losses.mean()\n",
        "  # modeli tekrar eğitim moduna geçir\n",
        "  model.train()\n",
        "  # eğitim ve doğrulama kayıplarını döndür\n",
        "  return out\n",
        "\n"
      ],
      "metadata": {
        "id": "yXiDNBzimVuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- @torch.no_grad(): Gradyan hesaplamasını devre dışı bırakır, böylece değerlendirme sırasında bellek ve hesaplama tasarrufu yapılır.\n",
        "- model.eval(): Modeli değerlendirme moduna alır (örneğin, dropout veya batch normalization gibi katmanlar farklı davranır)\n",
        "- split döngüsü: Hem eğitim (train) hem de doğrulama (val) setleri için kayıp hesaplanır\n",
        "- losses: eval_iters kadar kayıp değerini saklar. Her iterasyonda yeni bir batch alınır ve modelin kaybı hesaplanır\n",
        "- logits, loss: Model, giriş (X) ve hedef (Y) verileriyle çalıştırılır. logits, modelin ham tahminlerini; loss, tahminlerin ne kadar yanlış olduğunu gösterir.\n",
        "- losses.mean(): eval_iters boyunca hesaplanan kayıpların ortalaması alınır.\n",
        "- model.train(): Model, değerlendirme bittikten sonra tekrar eğitim moduna geçirilir.\n",
        "- Dönen değer: Eğitim ve doğrulama setleri için ortalama kayıpları içeren bir sözlük ({'train': ..., 'val': ...})"
      ],
      "metadata": {
        "id": "j9rdSEIWEXnC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Çok basit bir bigram dil modeli sınıfı\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "  #Modelin başlatılması\n",
        "  def __init__(self,vocab_size):\n",
        "    super().__init__()\n",
        "    # Her token , bir sonraki token için logits'i bir tablodan doğrudan okur\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size,vocab_size)\n",
        "\n",
        "  #Modelin ileri geçiş(forward pass) fonksiyonu\n",
        "  def forward(self,idx , targets = None):\n",
        "    # idx ve targets , tam sayılardan oluşan (B , T) boyutlu tensorlardır\n",
        "\n",
        "    logits = self.token_embedding_table(idx) # (B , T , C) boyutlu tensor haline gelir\n",
        "    # Eğer hedef verimezse kayıp hesaplanmas\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "\n",
        "    else:\n",
        "      # logits ve targers tensorlerini uygun şekle getir\n",
        "      B , T , C = logits.shape\n",
        "      logits = logits.view(B*T , C) # (B*T,C) şekline düzleştir --- Uzatıyoruz yukarıdan aşağıya\n",
        "      targets = targets.view(B*T)  # (B*T) şekline düzleştir\n",
        "      # Çapraz entropi kaybını hesapla\n",
        "      loss = F.cross_entropy(logits , targets)\n",
        "    # Logits ve kaybı döndür\n",
        "    return logits , loss\n",
        "\n",
        "  #metin üretmek için fonksiyon\n",
        "  def generate(self , idx,max_new_tokens):\n",
        "    #idx : mevcut bağlamdaki indekslerin (B , T)  boyutlu dizisi\n",
        "    # max_new_tokens : üretilecek maksimum yeni token sayısı\n",
        "    for _ in range(max_new_tokens): # Belirtilen sayıda yeni token üret\n",
        "\n",
        "      # tahminleri al\n",
        "      logits, loss = self(idx)\n",
        "      # sadece son zaman adımına odaklan\n",
        "      logits = logits[: , -1 ,:] # (B,C ) şekline getirir(son token'ın logitsleri)\n",
        "      # Olasılıklara dönüştürmek için softmax uygula\n",
        "      probs = F.softmax(logits , dim = -1)  # (B , C) , her token için olasılık dağılımı\n",
        "      # Olasılık dağılımından rastgele bir token seç\n",
        "      idx_next = torch.multinomial(probs , num_samples = 1) # (B,1) bir sonraki token\n",
        "      # Seçilen tokeni mevcut diziye ekle\n",
        "      idx = torch.cat((idx , idx_next) , dim = 1)  # (B , T + 1) , diziyi güncelle\n",
        "    # Üretilen token dizisini döndür\n",
        "    return idx\n",
        "\n"
      ],
      "metadata": {
        "id": "MQQYDrXfndgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Sınıf tanımı: BigramLanguageModel, PyTorch'un nn.Module sınıfından türetilir. Bu, bir sinir ağı modelidir.\n",
        "- init:\n",
        "  - vocab_size: Metindeki benzersiz karakter sayısı (önceki kodda hesaplandı).\n",
        "  - nn.Embedding(vocab_size, vocab_size): Her karakter için bir gömme (embedding) vektörü oluşturur. Bu, bir tablo gibi çalışır: her karakter (token), bir sonraki karakterin olasılık dağılımını temsil eden bir vektöre (logits) eşlenir.\n",
        "  - Örneğin, eğer vocab_size = 65, her karakter için 65 boyutlu bir vektör (her biri bir sonraki karakterin olasılığını temsil eder) oluşturulur.\n",
        "- forward:\n",
        "  - idx: Giriş tensoru, şekli (B, T) (batch_size, block_size). Her eleman, bir karakterin tamsayı kodlamasıdır.\n",
        "  - targets: Hedef tensoru, şekli (B, T). Bir sonraki karakterlerin tamsayı kodlamalarını içerir.\n",
        "  - logits: nn.Embedding tablosundan her giriş token’ı için bir vektör alınır. Çıktı şekli: (B, T, C) (batch_size, block_size, vocab_size).\n",
        "- Kayıp hesaplama:\n",
        "  - Eğer targets yoksa, sadece logits döndürülür (örneğin, tahmin yaparken).\n",
        "  - Eğer targets varsa:\n",
        "    - logits ve targets düzleştirilir (view ile) çünkü cross_entropy fonksiyonu 2D giriş bekler.\n",
        "    - logits: (B*T, C) (her token için olasılık vektörü).\n",
        "    - targets: (B*T) (her token için doğru sonraki karakter).\n",
        "    - F.cross_entropy: Modelin tahminleri (logits) ile gerçek değerler (targets) arasındaki kaybı hesaplar.\n",
        "    - Dönen değer: logits, loss (tahminler ve kayıp)."
      ],
      "metadata": {
        "id": "433EDGejXVyO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- self : BigramLanguageModel sınıfının bir örneği(model)\n",
        "- idx : Mevcut bağlam , şekli (B ,T ) ---> (batch_size , mevcut bağlam uzunluğu).Her eleman , bir karakterin tam sayı kodlamasıdır.\n",
        "- max_new_token : Kaç yeni karakter üretileceğini belirtir.\n",
        "- logits , loss : Model , bağlam(idx) ile çalıştırılır ve her token için bir sonraki tokenın olasılıkları(logits)döndürür.Kayıp burada kullanılmaz.\n",
        "- logits[: , -1 , :] : Sadece son zaman adımının logitsleri alınır.(Çünkü bigram modelinde bir sonraki tokeni tahmin ediyoruz).Şekil (B , C ) olur.---> (C : vocab_size)\n",
        "- F.softmax: Logits'leri olasılıklara çevirir. dim=-1, son boyutta (vocab_size) softmax uygular.\n",
        "- torch.multinomial: Olasılık dağılımından rastgele bir token seçer. Örneğin, 'a' için yüksek olasılık varsa, onu seçme ihtimali daha yüksektir.\n",
        "- torch.cat: Yeni seçilen token (idx_next) mevcut bağlama eklenir, böylece bağlam büyür.\n",
        "- Dönen değer: Üretilen token dizisi, şekli (B, T+max_new_tokens)."
      ],
      "metadata": {
        "id": "AmE6M0BEgkmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bigram sil modelini oluştur\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "# Modeli seçilen cihaza (GPU veya CPU) taşı\n",
        "m = model.to(device)\n",
        "#Pytorch optimizasyon aracını oluştur\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)"
      ],
      "metadata": {
        "id": "s4QmlyQop2Tt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- model: BigramLanguageModel sınıfından bir model örneği oluşturulur. vocab_size, metindeki benzersiz karakter sayısına dayanır (önceki kodda hesaplandı).\n",
        "- to(device): Model, GPU (cuda) veya CPU'ya taşınır, böylece eğitim donanıma uygun olur.\n",
        "- optimizer: AdamW optimizatörü, modelin parametrelerini (örn. token_embedding_table) güncellemek için kullanılır. learning_rate, hiperparametrelerden gelir (1e-2 = 0.01).\n",
        "- model.parameters(): Modelin öğrenilebilir parametrelerini (gömme tablosu) optimizatöre verir."
      ],
      "metadata": {
        "id": "JThwJtmSiTMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Eğitim için belirtilen iterasyon kadar döngü\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # Her eval_iterval iterasyonda kaybı değerlendir\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()  # eğitim ve doğrulama kayıplarını hesapa\n",
        "        # adım numarası ve kayıpları yazdır( 4 ondalık basamakla)\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # Eğitim verisinden bir veri yığını al\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # Kaybı hesapla\n",
        "    logits, loss = model(xb, yb) #modeli çalıştırarak tahmin ve kayıp al\n",
        "    optimizer.zero_grad(set_to_none=True) #Önceki gradyanları sıfırla\n",
        "    loss.backward() # Gradyanları hesapla\n",
        "    optimizer.step() # Model parametrelerini güncelle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmeNQdHgqHfo",
        "outputId": "19db0503-024c-43aa-9bee-44e3eeccd2ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.5847, val loss 4.5811\n",
            "step 300: train loss 2.7966, val loss 2.8134\n",
            "step 600: train loss 2.5460, val loss 2.5566\n",
            "step 900: train loss 2.5060, val loss 2.5202\n",
            "step 1200: train loss 2.4792, val loss 2.5025\n",
            "step 1500: train loss 2.4744, val loss 2.4939\n",
            "step 1800: train loss 2.4703, val loss 2.4842\n",
            "step 2100: train loss 2.4635, val loss 2.4940\n",
            "step 2400: train loss 2.4577, val loss 2.4914\n",
            "step 2700: train loss 2.4707, val loss 2.4876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- max_iters: Eğitim için toplam iterasyon sayısı (örneğin, 3000).\n",
        "- eval_interval: Her 300 iterasyonda bir, modelin performansı (estimate_loss ile) kontrol edilir ve eğitim/doğrulama kayıpları yazdırılır.\n",
        "get_batch('train'): Eğitim verisinden rastgele bir batch alınır (xb: giriş, yb: hedef).\n",
        "log    its, loss: Model, giriş (xb) ile çalıştırılır ve tahminler (logits) ile kayıp (loss) hesaplanır.\n",
        "- optimizer.zero_grad(): Önceki gradyanlar sıfırlanır, böylece yeni gradyanlar temiz bir şekilde hesaplanır.\n",
        "- loss.backward(): Kayıp üzerinden gradyanlar hesaplanır (geri yayılım).\n",
        "- optimizer.step(): Gradyanlar kullanılarak model parametreleri güncellenir (örneğin, gömme tablosu).\n",
        "- print: Her eval_interval adımda, eğitim ve doğrulama kayıpları yazdırılır, böylece modelin öğrenme süreci izlenir."
      ],
      "metadata": {
        "id": "I0S8yRr6j7_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# modelden metin üret\n",
        "context = torch.zeros((1,1), dtype = torch.long , device = device) # Başlangıç bağlamı  : boş bir token (sıfır)\n",
        "# 500 yeni tken üret ve kodlanmış tokenları metne çevir\n",
        "print(decode(m.generate(context , max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHdILeCZq5Re",
        "outputId": "f8a00470-3b7e-48dc-d561-7bbd0735632c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Whey nesee miesosent alls ols s. t he pe hqu neerete\n",
            "\n",
            "Ifo p bleyo laun mpest?\n",
            "KEYOProrant mepr ind theak twieakes t fusco then f rere ben bem h an INClelimare!\n",
            "\n",
            "FassordBunthin'stise ittoof nous nd t.\n",
            "Nof lerin.\n",
            "\n",
            "ourllaclors, the ou d tharellele glyorm\n",
            "Thikik se br,\n",
            "LQUThewouithod Gop\n",
            "BRicinth?\n",
            "ETouize wrattharoungie lastngoeyon a K:\n",
            "KENUCo nk malarse. de augenesard d, pilldy as, de macoorlf r st.\n",
            "O:\n",
            "G dindfoe pas t ardir o t war Cof jofucouo ERDiat twin f atul, I'rmendiouthangericotharkeen\n",
            "CLARI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- context: Başlangıç bağlamı, (1, 1) boyutlu bir tensor (tek batch, tek token). torch.zeros ile sıfırla başlatılır (örneğin, metnin başındaki \"boş\" bir karakter).\n",
        "- m.generate: Model, context ile başlayarak 500 yeni token üretir. generate fonksiyonu, her adımda bir sonraki tokenı tahmin eder ve bağlama ekler.\n",
        "[0].tolist(): Üretilen token dizisi, ilk batch’ten alınır ([0]) ve Python listesine çevrilir.\n",
        "- decode: Tamsayı token’ları ([0, 1, 2, ...]) metne çevrilir (örneğin, \"hello\"). Bu, önceki kodda tanımlı itos (integer to string) ile yapılır.\n",
        "- print: Üretilen metin ekrana yazdırılır."
      ],
      "metadata": {
        "id": "8g5JAiuCkZm-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bigram Modelinin Sınırlamaları:**\n",
        "\n",
        "- Bigram modeli, yalnızca bir önceki karaktere bakar, bu yüzden uzun vadeli bağlamları yakalayamaz (örneğin, cümle yapıları veya hikaye tutarlılığı).\n",
        "- Üretilen metin, genellikle rastgele ve anlamsız olabilir, çünkü model çok basittir."
      ],
      "metadata": {
        "id": "sua8i8yyndEl"
      }
    }
  ]
}